{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241f7b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting automated blog creation process...\n",
      "üìù Configuration: 5 papers, 7 days back\n",
      "üîç Categories: cs.AI, cs.LG, cs.CL, cs.CV, stat.ML\n",
      "üìä Fetched 5 papers, 5 from last 7 days\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Processing: KnowTrace: Bootstrapping Iterative Retrieval-Augmented Generation with Structured Knowledge Tracing"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Original Paper: [http://arxiv.org/abs/2505.20245v1](http://arxiv.org/abs/2505.20245v1)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Are you ready to dive into the cutting-edge world of artificial intelligence and knowledge tracing? A recent scientific paper titled \"KnowTrace: Bootstrapping Iterative Retrieval-Augmented Generation with Structured Knowledge Tracing\" by Rui Li and team introduces an innovative framework that could revolutionize how large language models handle complex multi-hop questions.\n",
       "\n",
       "In simpler terms, the researchers have developed a method called KnowTrace that helps large language models like those used in search engines and chatbots to better understand and process information. Traditional methods often struggle with accumulating external information and making sense of it, leading to inefficiencies and information overload. KnowTrace addresses this challenge by autonomously organizing relevant information into a structured knowledge graph, making it easier for the language model to infer and generate accurate responses.\n",
       "\n",
       "So, what does this mean for us in the real world? Imagine if your favorite search engine or virtual assistant could provide more accurate and relevant answers to your complex questions. With KnowTrace, these AI systems could potentially become even more efficient in understanding and processing information, leading to improved user experiences and better outcomes in various tasks.\n",
       "\n",
       "Furthermore, the reflective mechanism of knowledge backtracing introduced in KnowTrace allows the language model to learn from its own mistakes and improve over time. This self-bootstrapping process could potentially lead to continuous enhancement of AI systems without the need for external supervision, paving the way for more autonomous and intelligent machines.\n",
       "\n",
       "The experiments conducted by the researchers showed promising results, with KnowTrace consistently outperforming existing methods in multi-hop question answering tasks. The bootstrapped version of KnowTrace even further amplified these gains, showcasing the potential of this framework to significantly improve the performance of large language models in handling complex information retrieval tasks.\n",
       "\n",
       "In conclusion, KnowTrace presents an exciting advancement in the field of artificial intelligence, promising to enhance the capabilities of AI systems in processing and reasoning over complex information. With its potential real-world implications, this framework could potentially shape the future of AI technology and how we interact with intelligent systems. The possibilities are endless, and the journey towards smarter AI systems has just begun. Let's stay tuned for more groundbreaking developments in this fascinating field!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating DALL-E 3 image for: KnowTrace: Bootstrapping Iterative Retrieval-Augme...\n",
      "‚úÖ DALL-E 3 image generated successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-7trcesexcJK1ksLDJeczoh3z/user-feQ9FVoAjxgjl56JZH3J4u5L/img-p1KUsnWvOn6dgFPncLhAQdd7.png?st=2025-05-27T21%3A02%3A28Z&se=2025-05-27T23%3A02%3A28Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=52f8f7b3-ca8d-4b21-9807-8b9df114d84c&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-05-27T08%3A09%3A55Z&ske=2025-05-28T08%3A09%3A55Z&sks=b&skv=2024-08-04&sig=0wVQVYuIHX0Xsn/A7WQbYEYUuUF6BPZRV7nEnB/Mlms%3D\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading image...\n",
      "üì§ Creating new file: assets/images/2025-05-27-09640f99.png\n",
      "üì• Upload response: 201\n",
      "‚úÖ Image saved to GitHub: https://porkpy.github.io/research_blogger/assets/images/2025-05-27-09640f99.png\n",
      "‚è≥ Waiting for GitHub Pages to deploy image (60 seconds)...\n",
      "‚úÖ GitHub Pages image is accessible!\n",
      "Successfully created blog post on GitHub: https://porkpy.github.io/research_blogger/2025/05/27/09640f99/\n",
      "Waiting 30 seconds for link preview generation...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Threads Post:\n",
       "üîç Exciting new research alert! \"KnowTrace\" introduces a novel approach combining retrieval and generation models with structured knowledge tracing. This innovative method shows great promise in enhancing the efficiency and accuracy of information retrieval systems. Stay tuned for more updates on this game-changing development! üß†...\n",
       "\n",
       "#AI #ArtificialIntelligence #MachineLearning #DataScience #Latest #Research #Arxiv #OpenAI\n",
       "\n",
       "Read more: https://porkpy.github.io/research_blogger/2025/05/27/09640f99/"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Media Container Status Code: 200\n",
      "Media container created with ID: 18072611329923306\n",
      "Waiting 30 seconds before publishing...\n",
      "Publish Thread Status Code: 200\n",
      "Successfully posted to Threads with ID: 17910603471136724\n",
      "Successfully posted to Threads with image!\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Processing: On Path to Multimodal Historical Reasoning: HistBench and HistAgent"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Original Paper: [http://arxiv.org/abs/2505.20246v1](http://arxiv.org/abs/2505.20246v1)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Are you a history buff looking for a fresh take on how artificial intelligence (AI) can tackle historical reasoning? Well, buckle up because a team of researchers has just unveiled an exciting new benchmark, HistBench, and a specialized AI agent, HistAgent, designed to revolutionize how we engage with historical materials and questions.\n",
       "\n",
       "In a nutshell, HistBench consists of 414 carefully crafted questions that put AI's historical reasoning capabilities to the test. These questions cover a wide array of historical problems, from basic factual retrieval to in-depth interpretive analysis of manuscripts and images, even delving into interdisciplinary challenges involving archaeology, linguistics, and cultural history. What sets HistBench apart is its diverse range of tasks spanning 29 ancient and modern languages, various historical periods, and global regions.\n",
       "\n",
       "So, what did the researchers find? Well, it turns out that existing large language models (LLMs) and general-purpose agents struggled to perform well on HistBench. This led the team to develop HistAgent, a specialized AI equipped with tools tailored for historical research, such as optical character recognition (OCR), translation, archival search, and image understanding. When put to the test, HistAgent, based on GPT-4o, outperformed other AI models, achieving an accuracy of 27.54% pass@1 and 36.47% pass@2 on HistBench.\n",
       "\n",
       "What does this mean for the real world? Imagine a future where AI-powered tools can assist historians, researchers, and students in navigating complex historical datasets, deciphering ancient texts, and analyzing historical artifacts with unprecedented speed and accuracy. HistAgent could potentially revolutionize how we approach historical research, offering new insights and perspectives that were previously out of reach.\n",
       "\n",
       "In conclusion, this groundbreaking research paves the way for a new era of AI-powered historical reasoning, where specialized agents like HistAgent can bridge the gap between technology and the humanities. So, whether you're a history enthusiast or simply fascinated by the intersection of AI and history, keep an eye out for the exciting developments on the path to multimodal historical reasoning."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating DALL-E 3 image for: On Path to Multimodal Historical Reasoning: HistBe...\n",
      "‚úÖ DALL-E 3 image generated successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-7trcesexcJK1ksLDJeczoh3z/user-feQ9FVoAjxgjl56JZH3J4u5L/img-QcEf7DkZ3v5A3elcOaCVi3t1.png?st=2025-05-27T21%3A05%3A03Z&se=2025-05-27T23%3A05%3A03Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=52f8f7b3-ca8d-4b21-9807-8b9df114d84c&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-05-27T02%3A48%3A38Z&ske=2025-05-28T02%3A48%3A38Z&sks=b&skv=2024-08-04&sig=/SUUIu73saLDyzfSJQlVTj9IJfQjjBIelqGiGxAsPd0%3D\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading image...\n",
      "üì§ Creating new file: assets/images/2025-05-27-4d8e902e.png\n",
      "üì• Upload response: 201\n",
      "‚úÖ Image saved to GitHub: https://porkpy.github.io/research_blogger/assets/images/2025-05-27-4d8e902e.png\n",
      "‚è≥ Waiting for GitHub Pages to deploy image (60 seconds)...\n",
      "‚úÖ GitHub Pages image is accessible!\n",
      "Successfully created blog post on GitHub: https://porkpy.github.io/research_blogger/2025/05/27/4d8e902e/\n",
      "Waiting 30 seconds for link preview generation...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Threads Post:\n",
       "üîç Exciting new research alert! üìö This study introduces HistBench & HistAgent, paving the way for enhanced multimodal historical reasoning. By combining historical text analysis with visual AI, this innovative approach offers a fresh perspective on understanding our past. üåü #History #AI #ResearchImpact\n",
       "\n",
       "#AI #ArtificialIntelligence #MachineLearning #DataScience #Latest #Research #Arxiv #OpenAI\n",
       "\n",
       "Read more: https://porkpy.github.io/research_blogger/2025/05/27/4d8e902e/"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Media Container Status Code: 200\n",
      "Media container created with ID: 17870535966366930\n",
      "Waiting 30 seconds before publishing...\n",
      "Publish Thread Status Code: 200\n",
      "Successfully posted to Threads with ID: 17983986479689017\n",
      "Successfully posted to Threads with image!\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Processing: It's High Time: A Survey of Temporal Information Retrieval and Question Answering"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Original Paper: [http://arxiv.org/abs/2505.20243v1](http://arxiv.org/abs/2505.20243v1)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Time is a fascinating dimension that shapes how we interact with information, and a recent scientific paper titled \"It's High Time: A Survey of Temporal Information Retrieval and Question Answering\" delves deep into the world of Temporal Information Retrieval and Temporal Question Answering. Authored by Bhawna Piryani, Abdelrahman Abdullah, Jamshid Mozafari, Avishek Anand, and Adam Jatowt, this survey sheds light on how time influences the way we generate, retrieve, and make sense of data.\n",
       "\n",
       "In simple terms, the researchers explore how systems can effectively handle time-sensitive information, such as news articles, web archives, and knowledge bases. With the exponential growth of time-stamped content, it has become crucial for these systems to address challenges like detecting temporal intent, normalizing time expressions, ordering events, and reasoning over evolving or ambiguous facts. This research is not just about understanding the past or present; it's about predicting and navigating the future of information retrieval.\n",
       "\n",
       "The implications of this study are vast and impactful. Imagine a world where search engines can not only provide you with relevant information but also consider the context of time. For instance, a query about historical events would yield more accurate results if the system can interpret and organize time-specific data effectively. In fields like journalism, history, and social media, this research can revolutionize how we access and interpret time-sensitive information.\n",
       "\n",
       "Moreover, the paper discusses a range of approaches, from traditional methods to cutting-edge neural models like transformer models and Large Language Models (LLMs). These advancements in temporal language modeling and retrieval-augmented generation (RAG) open up new possibilities for improving the accuracy and efficiency of information retrieval systems.\n",
       "\n",
       "Overall, this survey highlights the importance of considering time as a critical factor in information retrieval and question answering. By reviewing various techniques and evaluating their effectiveness, the researchers pave the way for more robust and efficient systems that can handle the complexities of time-sensitive data across diverse domains.\n",
       "\n",
       "In a world where information is constantly evolving, understanding the temporal dimension is indeed high time!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating DALL-E 3 image for: It's High Time: A Survey of Temporal Information R...\n",
      "‚úÖ DALL-E 3 image generated successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-7trcesexcJK1ksLDJeczoh3z/user-feQ9FVoAjxgjl56JZH3J4u5L/img-1L7EdmQ9P4IlXcZUebfjxKcl.png?st=2025-05-27T21%3A07%3A42Z&se=2025-05-27T23%3A07%3A42Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=52f8f7b3-ca8d-4b21-9807-8b9df114d84c&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-05-27T08%3A42%3A31Z&ske=2025-05-28T08%3A42%3A31Z&sks=b&skv=2024-08-04&sig=AQU77DkVD1GUTxSmMmfdtOArMGRx3gDMIRLWc5ZvLxY%3D\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading image...\n",
      "üì§ Creating new file: assets/images/2025-05-27-386a77f2.png\n",
      "üì• Upload response: 201\n",
      "‚úÖ Image saved to GitHub: https://porkpy.github.io/research_blogger/assets/images/2025-05-27-386a77f2.png\n",
      "‚è≥ Waiting for GitHub Pages to deploy image (60 seconds)...\n",
      "‚úÖ GitHub Pages image is accessible!\n",
      "Successfully created blog post on GitHub: https://porkpy.github.io/research_blogger/2025/05/27/386a77f2/\n",
      "Waiting 30 seconds for link preview generation...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Threads Post:\n",
       "üï∞Ô∏è‚è≥ Dive into the world of temporal information retrieval and question answering with this fascinating survey! Researchers discuss the challenges and advancements in understanding time-based queries, paving the way for improved search engines and AI systems. Stay ahead of the curve with this insightful read! #science #research\n",
       "\n",
       "#AI #ArtificialIntelligence #MachineLearning #DataScience #Latest #Research #Arxiv #OpenAI\n",
       "\n",
       "Read more: https://porkpy.github.io/research_blogger/2025/05/27/386a77f2/"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Media Container Status Code: 200\n",
      "Media container created with ID: 17883119613296433\n",
      "Waiting 30 seconds before publishing...\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import openai\n",
    "import requests\n",
    "import base64\n",
    "import time\n",
    "import hashlib\n",
    "import json\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from IPython.display import display, Markdown, Image\n",
    "from requests.exceptions import RequestException\n",
    "from io import BytesIO\n",
    "\n",
    "# Import secrets from separate file (not committed to git)\n",
    "from secrets_config import (\n",
    "    OPENAI_API_KEY,\n",
    "    THREADS_USER_ID,\n",
    "    THREADS_ACCESS_TOKEN,\n",
    "    APP_SECRET,\n",
    "    GITHUB_TOKEN,\n",
    "    GITHUB_REPO,\n",
    "    GITHUB_PAGES_SITE,\n",
    "    FACEBOOK_ID,\n",
    "    INSTAGRAM_ACCESS_TOKEN\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# BLOG AUTOMATION CONFIGURATION - Modify these as needed\n",
    "# ============================================================================\n",
    "\n",
    "# arXiv paper settings\n",
    "ARXIV_CATEGORIES = [\"cs.AI\", \"cs.LG\", \"cs.CL\", \"cs.CV\", \"stat.ML\"]\n",
    "MAX_PAPERS_TO_PROCESS = 5       # Number of papers to fetch and potentially process\n",
    "DAYS_BACK_TO_SEARCH = 7         # How many days back to search for new papers\n",
    "\n",
    "# Content generation settings\n",
    "BLOG_POST_LENGTH = \"300-400\"    # Target word count for blog posts\n",
    "BLOG_POST_MAX_TOKENS = 500      # Max tokens for GPT response\n",
    "BLOG_POST_TEMPERATURE = 0.7     # Creativity level (0.0-1.0)\n",
    "\n",
    "# Threads post settings\n",
    "THREADS_MAX_CHARS = 350         # Max characters for main Threads text\n",
    "THREADS_HASHTAGS = \"#AI #ArtificialIntelligence #MachineLearning #DataScience #Latest #Research #Arxiv #OpenAI\"\n",
    "THREADS_WAIT_TIME = 30          # Seconds to wait before publishing\n",
    "THREADS_MAX_RETRIES = 3\n",
    "\n",
    "# Image generation settings (DALL-E 3)\n",
    "IMAGE_MODEL = \"dall-e-3\"        # \"dall-e-2\" or \"dall-e-3\"\n",
    "IMAGE_QUALITY = \"standard\"      # \"standard\" or \"hd\" (hd costs more)\n",
    "IMAGE_STYLE = \"natural\"         # \"natural\" or \"vivid\"\n",
    "IMAGE_SIZE = \"1024x1024\"        # \"1024x1024\", \"1792x1024\", or \"1024x1792\"\n",
    "\n",
    "# Processing settings\n",
    "SKIP_EXISTING_POSTS = True      # Skip papers that already have blog posts\n",
    "SAVE_IMAGES_TO_GITHUB = True    # Download and save images to prevent expiration\n",
    "LINK_PREVIEW_WAIT = 30          # Seconds to wait for link preview generation\n",
    "GITHUB_PAGES_IMAGE_WAIT = 60    # Seconds to wait for image deployment\n",
    "\n",
    "# Testing/debugging settings\n",
    "TEST_MODE = False               # Set to True to process only 1 paper for testing\n",
    "VERBOSE_OUTPUT = True           # Show detailed processing information\n",
    "\n",
    "# ============================================================================\n",
    "# Auto-adjust settings based on test mode\n",
    "# ============================================================================\n",
    "if TEST_MODE:\n",
    "    MAX_PAPERS_TO_PROCESS = 1\n",
    "    print(\"üß™ TEST MODE ENABLED - Processing only 1 paper\")\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def generate_harvard_reference(paper):\n",
    "    authors = paper.authors\n",
    "    \n",
    "    if len(authors) == 1:\n",
    "        author_str = authors[0].name\n",
    "    elif len(authors) == 2:\n",
    "        author_str = f\"{authors[0].name} and {authors[1].name}\"\n",
    "    else:\n",
    "        author_str = f\"{authors[0].name} et al.\"\n",
    "    \n",
    "    year = paper.published.year\n",
    "    title = paper.title\n",
    "    \n",
    "    reference = f\"{author_str} ({year}) '{title}', arXiv preprint arXiv:{paper.get_short_id()}.\"\n",
    "    \n",
    "    return reference\n",
    "\n",
    "def fetch_latest_papers(categories=None, max_results=None, days_back=None):\n",
    "    # Use config defaults if no parameters provided\n",
    "    if categories is None:\n",
    "        categories = ARXIV_CATEGORIES\n",
    "    if max_results is None:\n",
    "        max_results = MAX_PAPERS_TO_PROCESS\n",
    "    if days_back is None:\n",
    "        days_back = DAYS_BACK_TO_SEARCH\n",
    "        \n",
    "    client_arxiv = arxiv.Client()  # Renamed to avoid confusion with OpenAI client\n",
    "    cutoff_date = datetime.now(timezone.utc) - timedelta(days=days_back)\n",
    "    \n",
    "    category_query = \" OR \".join([f\"cat:{cat}\" for cat in categories])\n",
    "    \n",
    "    search = arxiv.Search(\n",
    "        query = f\"({category_query})\",\n",
    "        max_results = max_results,\n",
    "        sort_by = arxiv.SortCriterion.SubmittedDate\n",
    "    )\n",
    "    \n",
    "    results = list(client_arxiv.results(search))\n",
    "    recent_papers = [paper for paper in results if paper.published.replace(tzinfo=timezone.utc) > cutoff_date]\n",
    "    \n",
    "    if VERBOSE_OUTPUT:\n",
    "        print(f\"üìä Fetched {len(results)} papers, {len(recent_papers)} from last {days_back} days\")\n",
    "    \n",
    "    return recent_papers\n",
    "\n",
    "def generate_blog_post(paper):\n",
    "    authors = ', '.join([author.name for author in paper.authors])\n",
    "    prompt = f\"\"\"Write an engaging blog post about the following scientific paper:\n",
    "\n",
    "Title: {paper.title}\n",
    "Authors: {authors}\n",
    "Abstract: {paper.summary}\n",
    "\n",
    "The blog post should:\n",
    "1. Explain the main findings in simple terms\n",
    "2. Discuss potential real-world implications\n",
    "3. Be engaging and accessible to a general audience\n",
    "4. Be around {BLOG_POST_LENGTH} words long\n",
    "\n",
    "Blog Post:\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that writes engaging blog posts about scientific papers.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=BLOG_POST_MAX_TOKENS,\n",
    "            temperature=BLOG_POST_TEMPERATURE\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating blog post: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_threads_post(paper, blog_post_url):\n",
    "    prompt = f\"\"\"Create a short, engaging post for Threads (max {THREADS_MAX_CHARS} characters) about this scientific paper:\n",
    "    Title: {paper.title}\n",
    "    \n",
    "    Include a brief highlight of the research and its potential impact. \n",
    "    Do not include any hashtags or 'Read more' statements.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that creates engaging social media posts about scientific papers.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=100,\n",
    "            temperature=BLOG_POST_TEMPERATURE\n",
    "        )\n",
    "        threads_text = response.choices[0].message.content.strip().replace(\":\", \"\")\n",
    "        \n",
    "        full_post = f\"{threads_text}\\n\\n{THREADS_HASHTAGS}\\n\\nRead more: {blog_post_url}\"\n",
    "        \n",
    "        if len(full_post) > 500:\n",
    "            available_chars = 500 - len(THREADS_HASHTAGS) - len(blog_post_url) - 15\n",
    "            truncated_text = threads_text[:available_chars-3] + \"...\"\n",
    "            full_post = f\"{truncated_text}\\n\\n{THREADS_HASHTAGS}\\n\\nRead more: {blog_post_url}\"\n",
    "        \n",
    "        return full_post\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating Threads post: {e}\")\n",
    "        return None\n",
    "   \n",
    "def generate_ai_image(paper, threads_post):\n",
    "    \"\"\"\n",
    "    Generate an AI image using DALL-E 3 based on the research paper\n",
    "    \"\"\"\n",
    "    # Extract key concepts from both title and abstract for better understanding\n",
    "    title_words = paper.title.lower()\n",
    "    abstract_words = paper.summary.lower()\n",
    "    combined_text = f\"{title_words} {abstract_words}\"\n",
    "    \n",
    "    # Create a more detailed prompt based on the actual research content\n",
    "    # Look for specific technical terms and concepts\n",
    "    \n",
    "    visual_elements = []\n",
    "    domain_context = \"\"\n",
    "    \n",
    "    # Computer Vision / Image Processing\n",
    "    if any(word in combined_text for word in ['image', 'vision', 'visual', 'detection', 'segmentation', 'object', 'face', 'recognition']):\n",
    "        domain_context = \"computer vision and image processing\"\n",
    "        visual_elements.extend([\n",
    "            \"digital image grids with highlighted features\",\n",
    "            \"geometric detection boxes and annotations\",\n",
    "            \"layered visual processing pipelines\",\n",
    "            \"camera or sensor imagery with analytical overlays\"\n",
    "        ])\n",
    "    \n",
    "    # Natural Language Processing\n",
    "    elif any(word in combined_text for word in ['language', 'text', 'nlp', 'translation', 'sentiment', 'dialogue', 'conversation', 'llm']):\n",
    "        domain_context = \"natural language processing and text analysis\"\n",
    "        visual_elements.extend([\n",
    "            \"flowing text streams transforming between languages\",\n",
    "            \"word clouds with connecting semantic relationships\",\n",
    "            \"chat bubbles and conversation interfaces\",\n",
    "            \"linguistic trees and grammar structures\"\n",
    "        ])\n",
    "    \n",
    "    # Machine Learning / Neural Networks\n",
    "    elif any(word in combined_text for word in ['neural', 'network', 'learning', 'training', 'model', 'algorithm', 'optimization']):\n",
    "        domain_context = \"machine learning and neural networks\"\n",
    "        visual_elements.extend([\n",
    "            \"interconnected neural network nodes with flowing data\",\n",
    "            \"gradient flows and optimization landscapes\",\n",
    "            \"training data points clustering and separating\",\n",
    "            \"layered network architectures with information flow\"\n",
    "        ])\n",
    "    \n",
    "    # Robotics / Autonomous Systems\n",
    "    elif any(word in combined_text for word in ['robot', 'autonomous', 'control', 'manipulation', 'navigation', 'motion']):\n",
    "        domain_context = \"robotics and autonomous systems\"\n",
    "        visual_elements.extend([\n",
    "            \"robotic arms with precise movement trajectories\",\n",
    "            \"autonomous vehicles navigating environments\",\n",
    "            \"sensor data visualization around robotic systems\",\n",
    "            \"mechanical components with motion indicators\"\n",
    "        ])\n",
    "    \n",
    "    # Data Science / Analysis\n",
    "    elif any(word in combined_text for word in ['data', 'analysis', 'mining', 'clustering', 'classification', 'prediction', 'statistics']):\n",
    "        domain_context = \"data science and analytics\"\n",
    "        visual_elements.extend([\n",
    "            \"data visualization charts and graphs\",\n",
    "            \"clustering patterns and data point relationships\",\n",
    "            \"statistical distributions and trend lines\",\n",
    "            \"database connections and information flow diagrams\"\n",
    "        ])\n",
    "    \n",
    "    # Healthcare / Medical AI\n",
    "    elif any(word in combined_text for word in ['medical', 'health', 'diagnosis', 'patient', 'clinical', 'drug', 'disease']):\n",
    "        domain_context = \"medical AI and healthcare technology\"\n",
    "        visual_elements.extend([\n",
    "            \"medical scan imagery with AI analysis highlights\",\n",
    "            \"molecular structures and drug interactions\",\n",
    "            \"patient data flows and diagnostic pathways\",\n",
    "            \"healthcare monitoring interfaces and vital signs\"\n",
    "        ])\n",
    "    \n",
    "    # Quantum Computing\n",
    "    elif any(word in combined_text for word in ['quantum', 'qubit', 'entanglement', 'superposition']):\n",
    "        domain_context = \"quantum computing and quantum information\"\n",
    "        visual_elements.extend([\n",
    "            \"quantum state visualizations with wave functions\",\n",
    "            \"entangled particle representations\",\n",
    "            \"quantum circuit diagrams with gate operations\",\n",
    "            \"probabilistic quantum measurement outcomes\"\n",
    "        ])\n",
    "    \n",
    "    # Reinforcement Learning / Gaming\n",
    "    elif any(word in combined_text for word in ['reinforcement', 'reward', 'policy', 'agent', 'environment', 'game']):\n",
    "        domain_context = \"reinforcement learning and intelligent agents\"\n",
    "        visual_elements.extend([\n",
    "            \"agent-environment interaction loops\",\n",
    "            \"reward signal visualizations and policy maps\",\n",
    "            \"decision trees and action space exploration\",\n",
    "            \"learning progress and performance curves\"\n",
    "        ])\n",
    "    \n",
    "    # Default AI Research\n",
    "    else:\n",
    "        domain_context = \"artificial intelligence research\"\n",
    "        visual_elements.extend([\n",
    "            \"abstract AI concept representations\",\n",
    "            \"algorithmic flow diagrams\",\n",
    "            \"digital transformation processes\",\n",
    "            \"computational thinking visualizations\"\n",
    "        ])\n",
    "    \n",
    "    # Extract specific paper concepts to make it even more targeted\n",
    "    specific_concepts = []\n",
    "    \n",
    "    # Look for specific techniques or models mentioned\n",
    "    techniques = ['transformer', 'cnn', 'rnn', 'lstm', 'bert', 'gpt', 'diffusion', 'gan', 'vae', 'attention']\n",
    "    for tech in techniques:\n",
    "        if tech in combined_text:\n",
    "            specific_concepts.append(tech)\n",
    "    \n",
    "    # Look for application domains\n",
    "    applications = ['autonomous driving', 'medical imaging', 'speech recognition', 'recommendation', 'translation']\n",
    "    for app in applications:\n",
    "        if app.replace(' ', '') in combined_text.replace(' ', ''):\n",
    "            specific_concepts.append(app)\n",
    "    \n",
    "    # Create the enhanced prompt\n",
    "    base_elements = \", \".join(visual_elements[:2])  # Use first 2 visual elements\n",
    "    \n",
    "    concept_addition = \"\"\n",
    "    if specific_concepts:\n",
    "        concept_addition = f\", specifically highlighting {specific_concepts[0]} concepts\"\n",
    "    \n",
    "    # Extract a key insight from the title for visual focus\n",
    "    title_focus = \"\"\n",
    "    if any(word in title_words for word in ['novel', 'new', 'improved', 'efficient', 'robust']):\n",
    "        title_focus = \" showcasing innovation and advancement\"\n",
    "    elif any(word in title_words for word in ['multi', 'cross', 'joint', 'unified']):\n",
    "        title_focus = \" emphasizing integration and connectivity\"\n",
    "    elif any(word in title_words for word in ['real-time', 'fast', 'rapid', 'efficient']):\n",
    "        title_focus = \" conveying speed and efficiency\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Create a modern, sophisticated illustration representing {domain_context}. \n",
    "    The image should feature {base_elements}{concept_addition}{title_focus}.\n",
    "    \n",
    "    Use a professional color palette with deep blues, purples, and subtle gradients. \n",
    "    The style should be clean, minimalist, and technically accurate, suitable for \n",
    "    a research publication or technical blog. \n",
    "    \n",
    "    Avoid any text, human figures, or company logos. Show concrete, recognizable \n",
    "    technical elements and concepts that directly relate to {domain_context.split(' ')[0]} \n",
    "    research. Make the connection to the research topic immediately clear and visually \n",
    "    representative of the actual work being done.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"Generating DALL-E 3 image for: {paper.title[:50]}...\")\n",
    "        print(f\"üé® Domain: {domain_context}\")\n",
    "        if specific_concepts:\n",
    "            print(f\"üîç Concepts: {', '.join(specific_concepts[:2])}\")\n",
    "        \n",
    "        response = client.images.generate(\n",
    "            model=IMAGE_MODEL,\n",
    "            prompt=prompt,\n",
    "            n=1,\n",
    "            size=IMAGE_SIZE,\n",
    "            quality=IMAGE_QUALITY,\n",
    "            style=IMAGE_STYLE\n",
    "        )\n",
    "        \n",
    "        image_url = response.data[0].url\n",
    "        print(\"‚úÖ DALL-E 3 image generated successfully!\")\n",
    "        \n",
    "        # Display the image in the notebook (if running in Jupyter)\n",
    "        try:\n",
    "            from IPython.display import display, Image\n",
    "            display(Image(url=image_url))\n",
    "        except ImportError:\n",
    "            print(f\"Image URL: {image_url}\")\n",
    "        \n",
    "        return image_url\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating DALL-E 3 image: {e}\")\n",
    "        \n",
    "        # Fallback: try with a simpler but still specific prompt\n",
    "        try:\n",
    "            print(\"Trying with simplified prompt...\")\n",
    "            simple_prompt = f\"A clean, modern illustration of {domain_context}, minimalist style, blue and purple gradient, technical diagram aesthetic\"\n",
    "            \n",
    "            response = client.images.generate(\n",
    "                model=IMAGE_MODEL,\n",
    "                prompt=simple_prompt,\n",
    "                n=1,\n",
    "                size=IMAGE_SIZE,\n",
    "                quality=IMAGE_QUALITY\n",
    "            )\n",
    "            \n",
    "            image_url = response.data[0].url\n",
    "            print(\"‚úÖ Fallback image generated successfully!\")\n",
    "            return image_url\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå Fallback also failed: {e2}\")\n",
    "            return None\n",
    "\n",
    "def download_and_save_image(image_url, paper_short_id, date):\n",
    "    \"\"\"\n",
    "    Download the AI image and save it to GitHub repository\n",
    "    This prevents the image from expiring\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    import base64\n",
    "    \n",
    "    try:\n",
    "        # Download the image\n",
    "        print(\"Downloading image...\")\n",
    "        img_response = requests.get(image_url)\n",
    "        img_response.raise_for_status()\n",
    "        \n",
    "        # Create filename\n",
    "        image_filename = f\"assets/images/{date}-{paper_short_id}.png\"\n",
    "        \n",
    "        # Encode image for GitHub API\n",
    "        encoded_image = base64.b64encode(img_response.content).decode(\"utf-8\")\n",
    "        \n",
    "        # Upload to GitHub\n",
    "        url = f\"https://api.github.com/repos/{GITHUB_REPO}/contents/{image_filename}\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"token {GITHUB_TOKEN}\",\n",
    "            \"Accept\": \"application/vnd.github+json\",\n",
    "            \"X-GitHub-Api-Version\": \"2022-11-28\"  # Use the API version from docs\n",
    "        }\n",
    "        \n",
    "        # Always try to create as new file first (no sha parameter)\n",
    "        print(f\"üì§ Creating new file: {image_filename}\")\n",
    "        data = {\n",
    "            \"message\": f\"Add image for blog post {paper_short_id}\",\n",
    "            \"content\": encoded_image\n",
    "        }\n",
    "        \n",
    "        response = requests.put(url, headers=headers, json=data)\n",
    "        print(f\"üì• Upload response: {response.status_code}\")\n",
    "        \n",
    "        if response.status_code == 201:\n",
    "            # Successfully created new file\n",
    "            github_image_url = f\"https://{GITHUB_PAGES_SITE}/{image_filename}\"\n",
    "            print(f\"‚úÖ Image saved to GitHub: {github_image_url}\")\n",
    "            \n",
    "            # Wait for GitHub Pages to deploy the image\n",
    "            print(f\"‚è≥ Waiting for GitHub Pages to deploy image ({GITHUB_PAGES_IMAGE_WAIT} seconds)...\")\n",
    "            time.sleep(GITHUB_PAGES_IMAGE_WAIT)\n",
    "            \n",
    "            # Test if the image is accessible\n",
    "            try:\n",
    "                test_response = requests.head(github_image_url, timeout=10)\n",
    "                if test_response.status_code == 200:\n",
    "                    print(\"‚úÖ GitHub Pages image is accessible!\")\n",
    "                    return github_image_url\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è GitHub Pages image not ready (status: {test_response.status_code})\")\n",
    "                    print(\"üìé Using original OpenAI URL as fallback\")\n",
    "                    return image_url\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Cannot verify GitHub Pages image accessibility: {e}\")\n",
    "                print(\"üìé Using original OpenAI URL as fallback\")\n",
    "                return image_url\n",
    "                \n",
    "        elif response.status_code == 422:\n",
    "            # File might already exist, try to update it\n",
    "            print(\"üîÑ File might exist, checking and updating...\")\n",
    "            \n",
    "            # Get the existing file info\n",
    "            check_response = requests.get(url, headers=headers)\n",
    "            if check_response.status_code == 200:\n",
    "                file_info = check_response.json()\n",
    "                print(f\"üìÑ File exists, updating with sha: {file_info['sha'][:8]}...\")\n",
    "                \n",
    "                # Update with sha\n",
    "                update_data = {\n",
    "                    \"message\": f\"Update image for blog post {paper_short_id}\",\n",
    "                    \"content\": encoded_image,\n",
    "                    \"sha\": file_info[\"sha\"]\n",
    "                }\n",
    "                \n",
    "                update_response = requests.put(url, headers=headers, json=update_data)\n",
    "                if update_response.status_code == 200:\n",
    "                    github_image_url = f\"https://{GITHUB_PAGES_SITE}/{image_filename}\"\n",
    "                    print(f\"‚úÖ Image updated on GitHub: {github_image_url}\")\n",
    "                    return github_image_url\n",
    "                else:\n",
    "                    print(f\"‚ùå Failed to update: {update_response.status_code}\")\n",
    "                    print(f\"Response: {update_response.text}\")\n",
    "                    return image_url\n",
    "            else:\n",
    "                print(f\"‚ùå Cannot check file existence: {check_response.status_code}\")\n",
    "                print(\"üìé Using original OpenAI URL as fallback\")\n",
    "                return image_url\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to save image to GitHub: {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            print(\"üìé Using original OpenAI URL as fallback\")\n",
    "            return image_url\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving image: {e}\")\n",
    "        print(\"üìé Using original OpenAI URL as fallback\")\n",
    "        return image_url\n",
    "\n",
    "def create_github_blog_post(paper, content, date, short_id, image_url):\n",
    "    # Use consistent short_id for the file name\n",
    "    file_name = f\"{date}-{short_id}.md\"\n",
    "    \n",
    "    # Generate Harvard reference\n",
    "    harvard_reference = generate_harvard_reference(paper)\n",
    "    \n",
    "    # Fixed: No indentation in front matter\n",
    "    file_content = f\"\"\"---\n",
    "layout: post\n",
    "title: \"{paper.title}\"\n",
    "date: {date} {datetime.now().strftime('%H:%M:%S +0000')}\n",
    "categories: [blog, AI, research]\n",
    "image: {image_url}\n",
    "---\n",
    "![AI Generated Image]({image_url})\n",
    "\n",
    "{content}\n",
    "\n",
    "## Original Research Paper\n",
    "For more details, please refer to the original research paper:\n",
    "[{paper.title}]({paper.entry_id})\n",
    "\n",
    "## Reference\n",
    "{harvard_reference}\n",
    "\"\"\"\n",
    "    \n",
    "    encoded_content = base64.b64encode(file_content.encode(\"utf-8\")).decode(\"utf-8\")\n",
    "    url = f\"https://api.github.com/repos/{GITHUB_REPO}/contents/_posts/{file_name}\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"token {GITHUB_TOKEN}\",\n",
    "        \"Accept\": \"application/vnd.github.v3+json\"\n",
    "    }\n",
    "    \n",
    "    # Check if file already exists\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        print(f\"Blog post already exists: {file_name}\")\n",
    "        return False, \"\"\n",
    "\n",
    "    # File doesn't exist, create new file\n",
    "    data = {\n",
    "        \"message\": f\"Add new blog post: {paper.title}\",\n",
    "        \"content\": encoded_content\n",
    "    }\n",
    "\n",
    "    response = requests.put(url, headers=headers, json=data)\n",
    "    if response.status_code != 201:\n",
    "        print(f\"GitHub API Error: {response.status_code}\")\n",
    "        print(f\"Response content: {response.text}\")\n",
    "        return False, \"\"\n",
    "    \n",
    "    # Construct the URL based on the file name\n",
    "    post_url = f\"https://{GITHUB_PAGES_SITE}/{date.replace('-', '/')}/{short_id}/\"\n",
    "    return True, post_url\n",
    "   \n",
    "\n",
    "def check_existing_post(short_id, date):\n",
    "    file_name = f\"{date}-{short_id}.md\"\n",
    "    url = f\"https://api.github.com/repos/{GITHUB_REPO}/contents/_posts/{file_name}\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"token {GITHUB_TOKEN}\",\n",
    "        \"Accept\": \"application/vnd.github.v3+json\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return response.status_code == 200\n",
    "\n",
    "def create_media_container(access_token, user_id, text, image_url):\n",
    "    url = f\"https://graph.threads.net/v1.0/{user_id}/threads\"\n",
    "    \n",
    "    params = {\n",
    "        \"media_type\": \"IMAGE\",\n",
    "        \"image_url\": image_url,\n",
    "        \"text\": text,\n",
    "        \"access_token\": access_token\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        print(f\"Create Media Container Status Code: {response.status_code}\")\n",
    "        return response.json()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error creating media container: {e}\")\n",
    "        if hasattr(e, 'response') and e.response is not None:\n",
    "            print(f\"Response content: {e.response.text}\")\n",
    "        return None\n",
    "\n",
    "def publish_thread(access_token, user_id, creation_id):\n",
    "    url = f\"https://graph.threads.net/v1.0/{user_id}/threads_publish\"\n",
    "    \n",
    "    params = {\n",
    "        \"creation_id\": creation_id,\n",
    "        \"access_token\": access_token\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        print(f\"Publish Thread Status Code: {response.status_code}\")\n",
    "        return response.json()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error publishing thread: {e}\")\n",
    "        if hasattr(e, 'response') and e.response is not None:\n",
    "            print(f\"Response content: {e.response.text}\")\n",
    "        return None\n",
    "\n",
    "def post_to_threads(text, image_url, access_token, user_id, initial_wait=30, max_retries=3):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Step 1: Create media container\n",
    "            container = create_media_container(access_token, user_id, text, image_url)\n",
    "            if container is None or 'id' not in container:\n",
    "                print(\"Failed to create media container.\")\n",
    "                return False\n",
    "\n",
    "            container_id = container['id']\n",
    "            print(f\"Media container created with ID: {container_id}\")\n",
    "\n",
    "            # Wait before publishing\n",
    "            print(f\"Waiting {initial_wait} seconds before publishing...\")\n",
    "            time.sleep(initial_wait)\n",
    "\n",
    "            # Step 2: Publish the thread\n",
    "            publish_result = publish_thread(access_token, user_id, container_id)\n",
    "            if publish_result is None or 'id' not in publish_result:\n",
    "                print(\"Failed to publish thread.\")\n",
    "                return False\n",
    "\n",
    "            print(f\"Successfully posted to Threads with ID: {publish_result['id']}\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error posting to Threads: {e}\")\n",
    "            \n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Retrying in {initial_wait} seconds...\")\n",
    "                time.sleep(initial_wait)\n",
    "            else:\n",
    "                print(\"Max retries reached. Failed to post to Threads.\")\n",
    "                return False\n",
    "\n",
    "    return False\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        if VERBOSE_OUTPUT:\n",
    "            print(\"üöÄ Starting automated blog creation process...\")\n",
    "            print(f\"üìù Configuration: {MAX_PAPERS_TO_PROCESS} papers, {DAYS_BACK_TO_SEARCH} days back\")\n",
    "            print(f\"üîç Categories: {', '.join(ARXIV_CATEGORIES)}\")\n",
    "        \n",
    "        papers = fetch_latest_papers()\n",
    "        if not papers:\n",
    "            print(f\"No recent papers found in categories: {ARXIV_CATEGORIES}\")\n",
    "            return\n",
    "\n",
    "        processed_count = 0\n",
    "        for paper in papers:\n",
    "            display(Markdown(f\"## Processing: {paper.title}\"))\n",
    "            \n",
    "            short_id = hashlib.md5(paper.title.encode()).hexdigest()[:8]\n",
    "            \n",
    "            date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "            if SKIP_EXISTING_POSTS and check_existing_post(short_id, date):\n",
    "                print(f\"Blog post already exists for: {paper.title}\")\n",
    "                print(\"Skipping to next paper...\")\n",
    "                continue\n",
    "\n",
    "            blog_post = generate_blog_post(paper)\n",
    "            if not blog_post:\n",
    "                print(f\"Failed to generate blog post for: {paper.title}\")\n",
    "                print(\"Skipping to next paper...\")\n",
    "                continue\n",
    "\n",
    "            display(Markdown(f\"### Original Paper: [{paper.entry_id}]({paper.entry_id})\"))\n",
    "            display(Markdown(blog_post))\n",
    "            \n",
    "            # Generate a temporary post URL\n",
    "            temp_post_url = f\"https://{GITHUB_PAGES_SITE}/{date.replace('-', '/')}/{short_id}/\"\n",
    "            \n",
    "            # Generate Threads post first\n",
    "            threads_post = generate_threads_post(paper, temp_post_url)\n",
    "            if not threads_post:\n",
    "                print(\"Failed to generate Threads post. Skipping to next paper...\")\n",
    "                continue\n",
    "            \n",
    "            # Generate image based on paper content\n",
    "            image_url = generate_ai_image(paper, threads_post)\n",
    "            if not image_url:\n",
    "                print(\"Failed to generate AI image. Skipping to next paper...\")\n",
    "                continue\n",
    "            \n",
    "            # Download and save the image to prevent expiration\n",
    "            if SAVE_IMAGES_TO_GITHUB:\n",
    "                image_url = download_and_save_image(image_url, short_id, date)\n",
    "            \n",
    "            # Create GitHub blog post with the generated image\n",
    "            success, post_url = create_github_blog_post(paper, blog_post, date, short_id, image_url)\n",
    "            \n",
    "            if success:\n",
    "                print(f\"Successfully created blog post on GitHub: {post_url}\")\n",
    "                \n",
    "                # Add delay to allow for link preview generation\n",
    "                print(f\"Waiting {LINK_PREVIEW_WAIT} seconds for link preview generation...\")\n",
    "                time.sleep(LINK_PREVIEW_WAIT)\n",
    "                \n",
    "                # Update the Threads post with the correct URL if it changed\n",
    "                if post_url != temp_post_url:\n",
    "                    threads_post = threads_post.replace(temp_post_url, post_url)\n",
    "                \n",
    "                display(Markdown(f\"### Threads Post:\\n{threads_post}\"))\n",
    "                if post_to_threads(threads_post, image_url, THREADS_ACCESS_TOKEN, THREADS_USER_ID, THREADS_WAIT_TIME, THREADS_MAX_RETRIES):\n",
    "                    print(\"Successfully posted to Threads with image!\")\n",
    "                else:\n",
    "                    print(\"Failed to post to Threads.\")\n",
    "                \n",
    "                processed_count += 1\n",
    "            else:\n",
    "                print(\"Failed to create blog post on GitHub.\")\n",
    "        \n",
    "        if VERBOSE_OUTPUT:\n",
    "            print(f\"\\n‚úÖ Processing complete! Created {processed_count} new blog posts.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
