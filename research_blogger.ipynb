{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241f7b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting automated blog creation process...\n",
      "üìù Configuration: 5 papers, 7 days back\n",
      "üîç Categories: cs.AI, cs.LG, cs.CL, cs.CV, stat.ML\n",
      "üìä Fetched 5 papers, 5 from last 7 days\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Processing: HunyuanVideo-Avatar: High-Fidelity Audio-Driven Human Animation for Multiple Characters"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Original Paper: [http://arxiv.org/abs/2505.20156v1](http://arxiv.org/abs/2505.20156v1)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Are you ready to witness the future of audio-driven human animation? A groundbreaking study by Yi Chen and team introduces HunyuanVideo-Avatar, a cutting-edge model that brings multiple characters to life in high-fidelity dialogue videos.\n",
       "\n",
       "So, what does all this scientific jargon actually mean? Let's break it down. In simple terms, HunyuanVideo-Avatar is like a magician that can create dynamic, emotionally expressive avatars that talk and move realistically on screen. Imagine watching a movie where the characters not only speak but also convey their emotions through their facial expressions and body language with incredible accuracy.\n",
       "\n",
       "One of the key findings of this study is the development of a new technology called the multimodal diffusion transformer (MM-DiT), which allows for the seamless integration of audio and visual cues to generate lifelike animations. By using a character image injection module, the model ensures that each character maintains its unique traits consistently throughout the video, making the animations truly believable.\n",
       "\n",
       "But why is this research important beyond the realm of entertainment? The implications are vast and exciting. Imagine using this technology in virtual reality experiences to create interactive characters that respond to your voice commands and emotions in real-time. In education, it could revolutionize online learning by providing engaging and personalized avatars for virtual instructors. In the world of gaming, imagine playing a game where the characters not only look realistic but also react emotionally to your gameplay decisions.\n",
       "\n",
       "The possibilities are endless, and HunyuanVideo-Avatar opens up a whole new world of opportunities for immersive storytelling and interactive experiences. This study not only pushes the boundaries of audio-driven animation but also paves the way for future innovations in artificial intelligence and human-computer interaction.\n",
       "\n",
       "In a nutshell, HunyuanVideo-Avatar is not just a scientific breakthrough‚Äîit's a glimpse into a future where technology blurs the line between reality and imagination. Get ready to be amazed by the magic of high-fidelity audio-driven human animation and step into a world where your favorite characters come to life like never before."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating DALL-E 3 image for: HunyuanVideo-Avatar: High-Fidelity Audio-Driven Hu...\n",
      "‚úÖ DALL-E 3 image generated successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-7trcesexcJK1ksLDJeczoh3z/user-feQ9FVoAjxgjl56JZH3J4u5L/img-kqBOe8qSLBMC6CoRPjNln9zV.png?st=2025-05-27T20%3A23%3A34Z&se=2025-05-27T22%3A23%3A34Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=52f8f7b3-ca8d-4b21-9807-8b9df114d84c&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-05-27T12%3A26%3A42Z&ske=2025-05-28T12%3A26%3A42Z&sks=b&skv=2024-08-04&sig=mmoQjLgBjyLAMZLt9Gbd2cafowvFX9hKhtEJOp%2BPs9c%3D\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading image...\n",
      "‚úÖ Image saved to GitHub: https://porkpy.github.io/research_blogger/assets/images/2025-05-27-cd414e2d.png\n",
      "Successfully created blog post on GitHub: https://porkpy.github.io/research_blogger/2025/05/27/cd414e2d/\n",
      "Waiting 30 seconds for link preview generation...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Threads Post:\n",
       "üåü Exciting New Research Alert! üåü Scientists have developed HunyuanVideo-Avatar, a cutting-edge technology that generates realistic human animations for multiple characters using audio input. This breakthrough could revolutionize the field of animation and virtual communication! üé¨üîä #science #animation #technology\n",
       "\n",
       "#AI #ArtificialIntelligence #MachineLearning #DataScience #Latest #Research #Arxiv #OpenAI\n",
       "\n",
       "Read more: https://porkpy.github.io/research_blogger/2025/05/27/cd414e2d/"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating media container: 400 Client Error: Bad Request for url: https://graph.threads.net/v1.0/7610082715761011/threads?media_type=IMAGE&image_url=https%3A%2F%2Fporkpy.github.io%2Fresearch_blogger%2Fassets%2Fimages%2F2025-05-27-cd414e2d.png&text=%F0%9F%8C%9F+Exciting+New+Research+Alert%21+%F0%9F%8C%9F+Scientists+have+developed+HunyuanVideo-Avatar%2C+a+cutting-edge+technology+that+generates+realistic+human+animations+for+multiple+characters+using+audio+input.+This+breakthrough+could+revolutionize+the+field+of+animation+and+virtual+communication%21+%F0%9F%8E%AC%F0%9F%94%8A+%23science+%23animation+%23technology%0A%0A%23AI+%23ArtificialIntelligence+%23MachineLearning+%23DataScience+%23Latest+%23Research+%23Arxiv+%23OpenAI%0A%0ARead+more%3A+https%3A%2F%2Fporkpy.github.io%2Fresearch_blogger%2F2025%2F05%2F27%2Fcd414e2d%2F&access_token=THQVFUWGI1SWVlUmlaamlVQVdId3FqVlQ3aWtYNzgzeE4zbHg1bDB3SS1CZAjFBcGdVTXdmaks3ZAEVSSUZAjcExYR3VMTG5jSlZAKeWtkYmFTNUJ5QWJLdnRJMlZAMSDZA3NzJmTzZAjZA3lETGY0ZAzJGbHRmeEhFeno4ZADZAIcmcZD\n",
      "Response content: {\"error\":{\"message\":\"An unknown error occurred\",\"type\":\"OAuthException\",\"code\":1,\"error_subcode\":2207052,\"is_transient\":false,\"error_user_title\":\"Media download has failed. The media URI doesn't meet our requirements.\",\"error_user_msg\":\"The media could not be fetched from this URI: https:\\/\\/porkpy.github.io\\/research_blogger\\/assets\\/images\\/2025-05-27-cd414e2d.png. Please check the limitations section in our development document for more information: https:\\/\\/developers.facebook.com\\/docs\\/instagram-platform\\/instagram-graph-api\\/reference\\/ig-user\\/media#creating\",\"fbtrace_id\":\"AROZxBD7cW15t5h68Hd5RTo\"}}\n",
      "Failed to create media container.\n",
      "Failed to post to Threads.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Processing: Pangu Light: Weight Re-Initialization for Pruning and Accelerating LLMs"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Original Paper: [http://arxiv.org/abs/2505.20155v1](http://arxiv.org/abs/2505.20155v1)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Are you a fan of cutting-edge technology and artificial intelligence advancements? If so, you're in for a treat with the latest research on accelerating Large Language Models (LLMs) using a revolutionary technique called Pangu Light!\n",
       "\n",
       "In a nutshell, this scientific paper delves into the realm of LLMs, which are powerful models capable of performing a wide range of tasks with remarkable accuracy. However, these models come with a downside - their sheer size and computational demands can be a challenge for practical deployment. But fear not, researchers have come up with a solution: Pangu Light.\n",
       "\n",
       "So, what exactly is Pangu Light? It's a framework designed to accelerate LLMs by implementing structured pruning combined with innovative weight re-initialization techniques. By strategically re-initializing and adjusting the remaining weights after pruning, Pangu Light aims to improve the model's training accuracy post-pruning. This approach tackles the performance degradation often seen with aggressive pruning methods, making it possible to compress LLMs without sacrificing accuracy.\n",
       "\n",
       "But why is this important in the real world? Well, imagine using language models for tasks like natural language processing, text generation, or machine translation. By making these models more efficient and faster to deploy, Pangu Light could have a significant impact on various industries, from improving search engines to enhancing chatbots and automated customer service.\n",
       "\n",
       "What sets Pangu Light apart is its focus on multiple aspects of the model, including width, depth, attention heads, and RMSNorm. The framework introduces novel re-initialization methods like Cross-Layer Attention Pruning and Stabilized LayerNorm Pruning, which help mitigate performance drops and provide a better starting point for training.\n",
       "\n",
       "In practical terms, Pangu Light has shown superior accuracy and efficiency compared to existing pruning methods and established LLMs. For example, on Ascend NPUs, Pangu Light-32B outperformed Qwen3-32B in both average score and throughput, showcasing the potential of this groundbreaking approach.\n",
       "\n",
       "In conclusion, Pangu Light opens up exciting possibilities for enhancing the performance of LLMs while reducing computational costs. It's a step towards making advanced AI technologies more accessible and practical for a wide range of applications, paving the way for a future where language models can do more with less. Exciting times ahead in the world of artificial intelligence!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating DALL-E 3 image for: Pangu Light: Weight Re-Initialization for Pruning ...\n",
      "‚úÖ DALL-E 3 image generated successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-7trcesexcJK1ksLDJeczoh3z/user-feQ9FVoAjxgjl56JZH3J4u5L/img-ZpPmyijo2axx59aYHEhc5ybs.png?st=2025-05-27T20%3A24%3A30Z&se=2025-05-27T22%3A24%3A30Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=52f8f7b3-ca8d-4b21-9807-8b9df114d84c&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-05-27T16%3A58%3A16Z&ske=2025-05-28T16%3A58%3A16Z&sks=b&skv=2024-08-04&sig=uwg5Oy6XMipichqujapF2SX/DSq22/pTZBXoUbUiFO4%3D\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading image...\n",
      "‚úÖ Image saved to GitHub: https://porkpy.github.io/research_blogger/assets/images/2025-05-27-ddef2e34.png\n",
      "Successfully created blog post on GitHub: https://porkpy.github.io/research_blogger/2025/05/27/ddef2e34/\n",
      "Waiting 30 seconds for link preview generation...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Threads Post:\n",
       "üîç New research alert! Scientists introduce Pangu Light a method for re-initializing weights during pruning to speed up Large Language Models. This innovative approach could pave the way for more efficient and faster natural language processing models. #AI #Research #Innovation\n",
       "\n",
       "#AI #ArtificialIntelligence #MachineLearning #DataScience #Latest #Research #Arxiv #OpenAI\n",
       "\n",
       "Read more: https://porkpy.github.io/research_blogger/2025/05/27/ddef2e34/"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating media container: 400 Client Error: Bad Request for url: https://graph.threads.net/v1.0/7610082715761011/threads?media_type=IMAGE&image_url=https%3A%2F%2Fporkpy.github.io%2Fresearch_blogger%2Fassets%2Fimages%2F2025-05-27-ddef2e34.png&text=%F0%9F%94%8D+New+research+alert%21+Scientists+introduce+Pangu+Light+a+method+for+re-initializing+weights+during+pruning+to+speed+up+Large+Language+Models.+This+innovative+approach+could+pave+the+way+for+more+efficient+and+faster+natural+language+processing+models.+%23AI+%23Research+%23Innovation%0A%0A%23AI+%23ArtificialIntelligence+%23MachineLearning+%23DataScience+%23Latest+%23Research+%23Arxiv+%23OpenAI%0A%0ARead+more%3A+https%3A%2F%2Fporkpy.github.io%2Fresearch_blogger%2F2025%2F05%2F27%2Fddef2e34%2F&access_token=THQVFUWGI1SWVlUmlaamlVQVdId3FqVlQ3aWtYNzgzeE4zbHg1bDB3SS1CZAjFBcGdVTXdmaks3ZAEVSSUZAjcExYR3VMTG5jSlZAKeWtkYmFTNUJ5QWJLdnRJMlZAMSDZA3NzJmTzZAjZA3lETGY0ZAzJGbHRmeEhFeno4ZADZAIcmcZD\n",
      "Response content: {\"error\":{\"message\":\"An unknown error occurred\",\"type\":\"OAuthException\",\"code\":1,\"error_subcode\":2207052,\"is_transient\":false,\"error_user_title\":\"Media download has failed. The media URI doesn't meet our requirements.\",\"error_user_msg\":\"The media could not be fetched from this URI: https:\\/\\/porkpy.github.io\\/research_blogger\\/assets\\/images\\/2025-05-27-ddef2e34.png. Please check the limitations section in our development document for more information: https:\\/\\/developers.facebook.com\\/docs\\/instagram-platform\\/instagram-graph-api\\/reference\\/ig-user\\/media#creating\",\"fbtrace_id\":\"Akuy6BFFFO7zgG1gjwOMbNl\"}}\n",
      "Failed to create media container.\n",
      "Failed to post to Threads.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Processing: UORA: Uniform Orthogonal Reinitialization Adaptation in Parameter-Efficient Fine-Tuning of Large Models"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Original Paper: [http://arxiv.org/abs/2505.20154v1](http://arxiv.org/abs/2505.20154v1)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Are you ready to supercharge your Large Language Models (LLMs) with a cutting-edge fine-tuning approach? Buckle up, because a groundbreaking paper titled \"UORA: Uniform Orthogonal Reinitialization Adaptation in Parameter-Efficient Fine-Tuning of Large Models\" by Xueyan Zhang and team is here to revolutionize the world of model optimization!\n",
       "\n",
       "In a nutshell, this paper introduces a game-changing technique called Uniform Orthogonal Reinitialization Adaptation (UORA) that promises to enhance the performance of LLMs while drastically reducing the number of trainable parameters. Think of it as giving your model a turbo boost without overwhelming it with unnecessary complexity.\n",
       "\n",
       "So, what makes UORA stand out from the crowd? Unlike its predecessors like LoRA and VeRA, UORA leverages a smart low-rank approximation method and an interpolation-based reparametrization mechanism to selectively reinitialize specific rows and columns in frozen projection matrices. This clever strategy not only streamlines the fine-tuning process but also leads to significant savings in computational resources and storage requirements.\n",
       "\n",
       "But why should you care about all this technical jargon? Well, the implications of UORA extend far beyond the realm of academic research. Imagine the possibilities of having a more efficient and effective fine-tuning method for LLMs in real-world applications. From improving natural language processing tasks to enhancing image classification models, UORA opens up a world of opportunities for industries relying on artificial intelligence and machine learning.\n",
       "\n",
       "With comprehensive experiments showcasing UORA's prowess on various benchmarks like GLUE and E2E, it's clear that this novel approach is a game-changer in the field of model optimization. By offering a scalable and resource-efficient solution for fine-tuning LLMs, UORA paves the way for more streamlined and cost-effective AI development processes.\n",
       "\n",
       "In a nutshell, UORA is not just another scientific paper ‚Äì it's a game-changer that has the potential to shape the future of artificial intelligence and machine learning. So, if you're ready to take your LLMs to the next level, keep an eye on this groundbreaking research and get ready to unlock a new era of efficiency and performance in the world of large models!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating DALL-E 3 image for: UORA: Uniform Orthogonal Reinitialization Adaptati...\n",
      "‚úÖ DALL-E 3 image generated successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-7trcesexcJK1ksLDJeczoh3z/user-feQ9FVoAjxgjl56JZH3J4u5L/img-XQbmQkAFTyfX0fxnrmqp01Xv.png?st=2025-05-27T20%3A25%3A25Z&se=2025-05-27T22%3A25%3A25Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=52f8f7b3-ca8d-4b21-9807-8b9df114d84c&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-05-27T10%3A03%3A49Z&ske=2025-05-28T10%3A03%3A49Z&sks=b&skv=2024-08-04&sig=usvaslWWE/Z9gvBY4a0JPMadaW12w6R4GLbaw1WLlXI%3D\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading image...\n",
      "‚úÖ Image saved to GitHub: https://porkpy.github.io/research_blogger/assets/images/2025-05-27-6ff727f2.png\n",
      "Successfully created blog post on GitHub: https://porkpy.github.io/research_blogger/2025/05/27/6ff727f2/\n",
      "Waiting 30 seconds for link preview generation...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Threads Post:\n",
       "üåü New research alert! üìö Scientists developed UORA, a technique for efficient fine-tuning of large models. This method improves accuracy and speeds up retraining processes. Exciting advancements in deep learning on the horizon! üß†üî¨ #ScienceTwitter\n",
       "\n",
       "#AI #ArtificialIntelligence #MachineLearning #DataScience #Latest #Research #Arxiv #OpenAI\n",
       "\n",
       "Read more: https://porkpy.github.io/research_blogger/2025/05/27/6ff727f2/"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating media container: 400 Client Error: Bad Request for url: https://graph.threads.net/v1.0/7610082715761011/threads?media_type=IMAGE&image_url=https%3A%2F%2Fporkpy.github.io%2Fresearch_blogger%2Fassets%2Fimages%2F2025-05-27-6ff727f2.png&text=%F0%9F%8C%9F+New+research+alert%21+%F0%9F%93%9A+Scientists+developed+UORA%2C+a+technique+for+efficient+fine-tuning+of+large+models.+This+method+improves+accuracy+and+speeds+up+retraining+processes.+Exciting+advancements+in+deep+learning+on+the+horizon%21+%F0%9F%A7%A0%F0%9F%94%AC+%23ScienceTwitter%0A%0A%23AI+%23ArtificialIntelligence+%23MachineLearning+%23DataScience+%23Latest+%23Research+%23Arxiv+%23OpenAI%0A%0ARead+more%3A+https%3A%2F%2Fporkpy.github.io%2Fresearch_blogger%2F2025%2F05%2F27%2F6ff727f2%2F&access_token=THQVFUWGI1SWVlUmlaamlVQVdId3FqVlQ3aWtYNzgzeE4zbHg1bDB3SS1CZAjFBcGdVTXdmaks3ZAEVSSUZAjcExYR3VMTG5jSlZAKeWtkYmFTNUJ5QWJLdnRJMlZAMSDZA3NzJmTzZAjZA3lETGY0ZAzJGbHRmeEhFeno4ZADZAIcmcZD\n",
      "Response content: {\"error\":{\"message\":\"An unknown error occurred\",\"type\":\"OAuthException\",\"code\":1,\"error_subcode\":2207052,\"is_transient\":false,\"error_user_title\":\"Media download has failed. The media URI doesn't meet our requirements.\",\"error_user_msg\":\"The media could not be fetched from this URI: https:\\/\\/porkpy.github.io\\/research_blogger\\/assets\\/images\\/2025-05-27-6ff727f2.png. Please check the limitations section in our development document for more information: https:\\/\\/developers.facebook.com\\/docs\\/instagram-platform\\/instagram-graph-api\\/reference\\/ig-user\\/media#creating\",\"fbtrace_id\":\"AIYkKL1WMM4VCumpdRz061b\"}}\n",
      "Failed to create media container.\n",
      "Failed to post to Threads.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Processing: Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Original Paper: [http://arxiv.org/abs/2505.20152v1](http://arxiv.org/abs/2505.20152v1)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Are you tired of struggling with complex geometric problems? Well, fear not, because a group of brilliant researchers has just unveiled a groundbreaking solution in the form of their latest scientific paper: \"Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models.\"\n",
       "\n",
       "In simple terms, the study focuses on enhancing the ability of computer models to understand and solve intricate geometric problems by developing a new learning framework called hard negative contrastive learning. By training visual encoders using this method, the researchers were able to significantly improve the performance of Large Multimodal Models (LMMs) in geometric reasoning tasks.\n",
       "\n",
       "So, what does this mean for the real world? Imagine a future where computers can effortlessly tackle complex geometric challenges, from designing intricate structures to analyzing spatial relationships in medical imaging. This advancement has the potential to revolutionize industries such as architecture, engineering, healthcare, and more, by providing powerful tools that can assist humans in solving intricate geometric problems with precision and efficiency.\n",
       "\n",
       "The key takeaway from this study is the significant improvement in geometric reasoning performance achieved by the model, MMGeoLM, which outperformed other existing models on three different geometric reasoning benchmarks. Even more impressively, despite its large size, MMGeoLM was able to compete with powerful closed-source models like GPT-4o.\n",
       "\n",
       "Moreover, the study also delves into the impact of different methods of constructing negative samples and the number of negative samples on the performance of LMMs in geometric reasoning tasks. These findings provide valuable insights into how to further enhance the capabilities of multimodal models in the realm of geometric understanding.\n",
       "\n",
       "In conclusion, this research opens up exciting possibilities for the future of artificial intelligence and its applications in solving complex geometric problems. By pushing the boundaries of contrastive learning and multimodal models, the scientists behind this study have paved the way for a new era of fine-grained geometric understanding. Who knows what incredible feats of problem-solving these advanced models will achieve next? The possibilities are as vast and limitless as the geometric shapes they aim to comprehend."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating DALL-E 3 image for: Hard Negative Contrastive Learning for Fine-Graine...\n",
      "‚úÖ DALL-E 3 image generated successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-7trcesexcJK1ksLDJeczoh3z/user-feQ9FVoAjxgjl56JZH3J4u5L/img-kTSRg3XD2tbZI4ISMJ6jylgU.png?st=2025-05-27T20%3A26%3A23Z&se=2025-05-27T22%3A26%3A23Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=52f8f7b3-ca8d-4b21-9807-8b9df114d84c&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-05-27T05%3A04%3A22Z&ske=2025-05-28T05%3A04%3A22Z&sks=b&skv=2024-08-04&sig=NrvFyq9Q1H/UpktynzS6D7LJ2eRu5CmlmK92P8R8jTA%3D\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading image...\n",
      "‚úÖ Image saved to GitHub: https://porkpy.github.io/research_blogger/assets/images/2025-05-27-46a6ff09.png\n",
      "Successfully created blog post on GitHub: https://porkpy.github.io/research_blogger/2025/05/27/46a6ff09/\n",
      "Waiting 30 seconds for link preview generation...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Threads Post:\n",
       "üîç Exciting new research alert! A study introduces Hard Negative Contrastive Learning, improving fine-grained geometric understanding in large multimodal models. This method enhances model performance, paving the way for more accurate and detailed data analysis. üåü #ScienceTwitter #ResearchHighlight\n",
       "\n",
       "#AI #ArtificialIntelligence #MachineLearning #DataScience #Latest #Research #Arxiv #OpenAI\n",
       "\n",
       "Read more: https://porkpy.github.io/research_blogger/2025/05/27/46a6ff09/"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating media container: 400 Client Error: Bad Request for url: https://graph.threads.net/v1.0/7610082715761011/threads?media_type=IMAGE&image_url=https%3A%2F%2Fporkpy.github.io%2Fresearch_blogger%2Fassets%2Fimages%2F2025-05-27-46a6ff09.png&text=%F0%9F%94%8D+Exciting+new+research+alert%21+A+study+introduces+Hard+Negative+Contrastive+Learning%2C+improving+fine-grained+geometric+understanding+in+large+multimodal+models.+This+method+enhances+model+performance%2C+paving+the+way+for+more+accurate+and+detailed+data+analysis.+%F0%9F%8C%9F+%23ScienceTwitter+%23ResearchHighlight%0A%0A%23AI+%23ArtificialIntelligence+%23MachineLearning+%23DataScience+%23Latest+%23Research+%23Arxiv+%23OpenAI%0A%0ARead+more%3A+https%3A%2F%2Fporkpy.github.io%2Fresearch_blogger%2F2025%2F05%2F27%2F46a6ff09%2F&access_token=THQVFUWGI1SWVlUmlaamlVQVdId3FqVlQ3aWtYNzgzeE4zbHg1bDB3SS1CZAjFBcGdVTXdmaks3ZAEVSSUZAjcExYR3VMTG5jSlZAKeWtkYmFTNUJ5QWJLdnRJMlZAMSDZA3NzJmTzZAjZA3lETGY0ZAzJGbHRmeEhFeno4ZADZAIcmcZD\n",
      "Response content: {\"error\":{\"message\":\"An unknown error occurred\",\"type\":\"OAuthException\",\"code\":1,\"error_subcode\":2207052,\"is_transient\":false,\"error_user_title\":\"Media download has failed. The media URI doesn't meet our requirements.\",\"error_user_msg\":\"The media could not be fetched from this URI: https:\\/\\/porkpy.github.io\\/research_blogger\\/assets\\/images\\/2025-05-27-46a6ff09.png. Please check the limitations section in our development document for more information: https:\\/\\/developers.facebook.com\\/docs\\/instagram-platform\\/instagram-graph-api\\/reference\\/ig-user\\/media#creating\",\"fbtrace_id\":\"ACjOdMEMmrDIRGYCV4sW6SY\"}}\n",
      "Failed to create media container.\n",
      "Failed to post to Threads.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Processing: On the (Non) Injectivity of Piecewise Linear Janossy Pooling"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Original Paper: [http://arxiv.org/abs/2505.20150v1](http://arxiv.org/abs/2505.20150v1)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Are you tired of complex neural network designs that promise improved performance but come at the cost of higher compute time? Well, a recent scientific paper by Ilai Reshef and Nadav Dym might have just the answer for you! In their study titled \"On the (Non) Injectivity of Piecewise Linear Janossy Pooling,\" the researchers dive deep into the world of multiset functions and shed light on an intriguing discovery.\n",
       "\n",
       "So, what exactly did Reshef and Dym uncover in their research? Let's break it down into simpler terms. Multiset functions play a crucial role in the development of neural networks for handling multisets and graphs. These functions map multisets (collections of items where duplicates are allowed) to vectors, providing a unique representation for each multiset. One key property desired in these mappings is injectivity, which ensures that each multiset is uniquely represented by a vector.\n",
       "\n",
       "In their study, the researchers focused on a specific type of multiset function known as k-ary Janossy pooling, a popular choice in the realm of multiset models. They set out to investigate whether piecewise linear Janossy pooling functions could achieve injectivity. The result? A surprising finding that no piecewise linear Janossy pooling function can be injective.\n",
       "\n",
       "But fear not, as there's a silver lining to this discovery! Reshef and Dym also demonstrated that even simple deep-sets models, when applied to multisets without multiplicities, can still achieve the desired properties of injectivity and bi-Lipschitzness. This opens up the possibility of using more straightforward multiset functions without compromising on the integrity of the representations.\n",
       "\n",
       "So, what does this mean for the real world? Well, the implications of this research could lead to more efficient neural network designs that strike a balance between performance and computational resources. By exploring simpler alternatives to complex multiset functions, researchers and developers may find ways to streamline their models without sacrificing accuracy.\n",
       "\n",
       "In conclusion, Reshef and Dym's paper challenges the status quo in multiset function design and paves the way for new possibilities in the realm of neural networks. Who knew that the quest for simplicity could uncover such fascinating insights in the world of computational science?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating DALL-E 3 image for: On the (Non) Injectivity of Piecewise Linear Janos...\n",
      "‚úÖ DALL-E 3 image generated successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-7trcesexcJK1ksLDJeczoh3z/user-feQ9FVoAjxgjl56JZH3J4u5L/img-SbFzUnV5tkpIFln1UWesfQzG.png?st=2025-05-27T20%3A27%3A15Z&se=2025-05-27T22%3A27%3A15Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=52f8f7b3-ca8d-4b21-9807-8b9df114d84c&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-05-27T18%3A09%3A00Z&ske=2025-05-28T18%3A09%3A00Z&sks=b&skv=2024-08-04&sig=ZzvD3mYT9J0rgkznynrllJCJ%2BJ6VRu4kvxZfCKlmHRM%3D\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading image...\n",
      "‚úÖ Image saved to GitHub: https://porkpy.github.io/research_blogger/assets/images/2025-05-27-6e617040.png\n",
      "Successfully created blog post on GitHub: https://porkpy.github.io/research_blogger/2025/05/27/6e617040/\n",
      "Waiting 30 seconds for link preview generation...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Threads Post:\n",
       "üî¨ New research on Piecewise Linear Janossy Pooling reveals surprising insights on its injectivity! This study challenges traditional assumptions and opens up exciting possibilities for improved data processing methods. Stay ahead of the curve with the latest findings in computational science! üåü #research #science #dataanalysis\n",
       "\n",
       "#AI #ArtificialIntelligence #MachineLearning #DataScience #Latest #Research #Arxiv #OpenAI\n",
       "\n",
       "Read more: https://porkpy.github.io/research_blogger/2025/05/27/6e617040/"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating media container: 400 Client Error: Bad Request for url: https://graph.threads.net/v1.0/7610082715761011/threads?media_type=IMAGE&image_url=https%3A%2F%2Fporkpy.github.io%2Fresearch_blogger%2Fassets%2Fimages%2F2025-05-27-6e617040.png&text=%F0%9F%94%AC+New+research+on+Piecewise+Linear+Janossy+Pooling+reveals+surprising+insights+on+its+injectivity%21+This+study+challenges+traditional+assumptions+and+opens+up+exciting+possibilities+for+improved+data+processing+methods.+Stay+ahead+of+the+curve+with+the+latest+findings+in+computational+science%21+%F0%9F%8C%9F+%23research+%23science+%23dataanalysis%0A%0A%23AI+%23ArtificialIntelligence+%23MachineLearning+%23DataScience+%23Latest+%23Research+%23Arxiv+%23OpenAI%0A%0ARead+more%3A+https%3A%2F%2Fporkpy.github.io%2Fresearch_blogger%2F2025%2F05%2F27%2F6e617040%2F&access_token=THQVFUWGI1SWVlUmlaamlVQVdId3FqVlQ3aWtYNzgzeE4zbHg1bDB3SS1CZAjFBcGdVTXdmaks3ZAEVSSUZAjcExYR3VMTG5jSlZAKeWtkYmFTNUJ5QWJLdnRJMlZAMSDZA3NzJmTzZAjZA3lETGY0ZAzJGbHRmeEhFeno4ZADZAIcmcZD\n",
      "Response content: {\"error\":{\"message\":\"An unknown error occurred\",\"type\":\"OAuthException\",\"code\":1,\"error_subcode\":2207052,\"is_transient\":false,\"error_user_title\":\"Media download has failed. The media URI doesn't meet our requirements.\",\"error_user_msg\":\"The media could not be fetched from this URI: https:\\/\\/porkpy.github.io\\/research_blogger\\/assets\\/images\\/2025-05-27-6e617040.png. Please check the limitations section in our development document for more information: https:\\/\\/developers.facebook.com\\/docs\\/instagram-platform\\/instagram-graph-api\\/reference\\/ig-user\\/media#creating\",\"fbtrace_id\":\"Aeg2Q5fmmCReBpP0hDkx9p2\"}}\n",
      "Failed to create media container.\n",
      "Failed to post to Threads.\n",
      "\n",
      "‚úÖ Processing complete! Created 5 new blog posts.\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import openai\n",
    "import requests\n",
    "import base64\n",
    "import time\n",
    "import hashlib\n",
    "import json\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from IPython.display import display, Markdown, Image\n",
    "from requests.exceptions import RequestException\n",
    "from io import BytesIO\n",
    "\n",
    "# Import secrets from separate file (not committed to git)\n",
    "from secrets_config import (\n",
    "    OPENAI_API_KEY,\n",
    "    THREADS_USER_ID,\n",
    "    THREADS_ACCESS_TOKEN,\n",
    "    APP_SECRET,\n",
    "    GITHUB_TOKEN,\n",
    "    GITHUB_REPO,\n",
    "    GITHUB_PAGES_SITE,\n",
    "    FACEBOOK_ID,\n",
    "    INSTAGRAM_ACCESS_TOKEN\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# BLOG AUTOMATION CONFIGURATION - Modify these as needed\n",
    "# ============================================================================\n",
    "\n",
    "# arXiv paper settings\n",
    "ARXIV_CATEGORIES = [\"cs.AI\", \"cs.LG\", \"cs.CL\", \"cs.CV\", \"stat.ML\"]\n",
    "MAX_PAPERS_TO_PROCESS = 5       # Number of papers to fetch and potentially process\n",
    "DAYS_BACK_TO_SEARCH = 7         # How many days back to search for new papers\n",
    "\n",
    "# Content generation settings\n",
    "BLOG_POST_LENGTH = \"300-400\"    # Target word count for blog posts\n",
    "BLOG_POST_MAX_TOKENS = 500      # Max tokens for GPT response\n",
    "BLOG_POST_TEMPERATURE = 0.7     # Creativity level (0.0-1.0)\n",
    "\n",
    "# Threads post settings\n",
    "THREADS_MAX_CHARS = 350         # Max characters for main Threads text\n",
    "THREADS_HASHTAGS = \"#AI #ArtificialIntelligence #MachineLearning #DataScience #Latest #Research #Arxiv #OpenAI\"\n",
    "THREADS_WAIT_TIME = 30          # Seconds to wait before publishing\n",
    "THREADS_MAX_RETRIES = 3\n",
    "\n",
    "# Image generation settings (DALL-E 3)\n",
    "IMAGE_MODEL = \"dall-e-3\"        # \"dall-e-2\" or \"dall-e-3\"\n",
    "IMAGE_QUALITY = \"standard\"      # \"standard\" or \"hd\" (hd costs more)\n",
    "IMAGE_STYLE = \"natural\"         # \"natural\" or \"vivid\"\n",
    "IMAGE_SIZE = \"1024x1024\"        # \"1024x1024\", \"1792x1024\", or \"1024x1792\"\n",
    "\n",
    "# Processing settings\n",
    "SKIP_EXISTING_POSTS = True      # Skip papers that already have blog posts\n",
    "SAVE_IMAGES_TO_GITHUB = True    # Download and save images to prevent expiration\n",
    "LINK_PREVIEW_WAIT = 30          # Seconds to wait for link preview generation\n",
    "GITHUB_PAGES_IMAGE_WAIT = 60    # Seconds to wait for image deployment\n",
    "\n",
    "# Testing/debugging settings\n",
    "TEST_MODE = False               # Set to True to process only 1 paper for testing\n",
    "VERBOSE_OUTPUT = True           # Show detailed processing information\n",
    "\n",
    "# ============================================================================\n",
    "# Auto-adjust settings based on test mode\n",
    "# ============================================================================\n",
    "if TEST_MODE:\n",
    "    MAX_PAPERS_TO_PROCESS = 1\n",
    "    print(\"üß™ TEST MODE ENABLED - Processing only 1 paper\")\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def generate_harvard_reference(paper):\n",
    "    authors = paper.authors\n",
    "    \n",
    "    if len(authors) == 1:\n",
    "        author_str = authors[0].name\n",
    "    elif len(authors) == 2:\n",
    "        author_str = f\"{authors[0].name} and {authors[1].name}\"\n",
    "    else:\n",
    "        author_str = f\"{authors[0].name} et al.\"\n",
    "    \n",
    "    year = paper.published.year\n",
    "    title = paper.title\n",
    "    \n",
    "    reference = f\"{author_str} ({year}) '{title}', arXiv preprint arXiv:{paper.get_short_id()}.\"\n",
    "    \n",
    "    return reference\n",
    "\n",
    "def fetch_latest_papers(categories, max_results=12):  # Fixed: was max_results=1\n",
    "    client_arxiv = arxiv.Client()  # Renamed to avoid confusion with OpenAI client\n",
    "    last_week = datetime.now(timezone.utc) - timedelta(days=7)\n",
    "    \n",
    "    category_query = \" OR \".join([f\"cat:{cat}\" for cat in categories])\n",
    "    \n",
    "    search = arxiv.Search(\n",
    "        query = f\"({category_query})\",\n",
    "        max_results = max_results,\n",
    "        sort_by = arxiv.SortCriterion.SubmittedDate\n",
    "    )\n",
    "    \n",
    "    results = list(client_arxiv.results(search))\n",
    "    return [paper for paper in results if paper.published.replace(tzinfo=timezone.utc) > last_week]\n",
    "\n",
    "def generate_blog_post(paper):\n",
    "    authors = ', '.join([author.name for author in paper.authors])\n",
    "    prompt = f\"\"\"Write an engaging blog post about the following scientific paper:\n",
    "\n",
    "Title: {paper.title}\n",
    "Authors: {authors}\n",
    "Abstract: {paper.summary}\n",
    "\n",
    "The blog post should:\n",
    "1. Explain the main findings in simple terms\n",
    "2. Discuss potential real-world implications\n",
    "3. Be engaging and accessible to a general audience\n",
    "4. Be around {BLOG_POST_LENGTH} words long\n",
    "\n",
    "Blog Post:\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that writes engaging blog posts about scientific papers.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=BLOG_POST_MAX_TOKENS,\n",
    "            temperature=BLOG_POST_TEMPERATURE\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating blog post: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_threads_post(paper, blog_post_url):\n",
    "    prompt = f\"\"\"Create a short, engaging post for Threads (max {THREADS_MAX_CHARS} characters) about this scientific paper:\n",
    "    Title: {paper.title}\n",
    "    \n",
    "    Include a brief highlight of the research and its potential impact. \n",
    "    Do not include any hashtags or 'Read more' statements.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that creates engaging social media posts about scientific papers.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=100,\n",
    "            temperature=BLOG_POST_TEMPERATURE\n",
    "        )\n",
    "        threads_text = response.choices[0].message.content.strip().replace(\":\", \"\")\n",
    "        \n",
    "        full_post = f\"{threads_text}\\n\\n{THREADS_HASHTAGS}\\n\\nRead more: {blog_post_url}\"\n",
    "        \n",
    "        if len(full_post) > 500:\n",
    "            available_chars = 500 - len(THREADS_HASHTAGS) - len(blog_post_url) - 15\n",
    "            truncated_text = threads_text[:available_chars-3] + \"...\"\n",
    "            full_post = f\"{truncated_text}\\n\\n{THREADS_HASHTAGS}\\n\\nRead more: {blog_post_url}\"\n",
    "        \n",
    "        return full_post\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating Threads post: {e}\")\n",
    "        return None\n",
    "   \n",
    "def generate_ai_image(paper, threads_post):\n",
    "    \"\"\"\n",
    "    Generate an AI image using DALL-E 3 based on the research paper\n",
    "    \"\"\"\n",
    "    # Create a more specific prompt based on the paper's content\n",
    "    # Extract key concepts from the paper title and abstract\n",
    "    title_words = paper.title.lower()\n",
    "    \n",
    "    # Determine the research domain for better image prompts\n",
    "    if any(word in title_words for word in ['neural', 'network', 'deep', 'learning', 'ai', 'artificial']):\n",
    "        domain = \"neural networks and AI\"\n",
    "        visual_style = \"futuristic digital networks with glowing nodes and connections\"\n",
    "    elif any(word in title_words for word in ['computer', 'vision', 'image', 'visual']):\n",
    "        domain = \"computer vision\"\n",
    "        visual_style = \"digital image processing with geometric patterns and visual data\"\n",
    "    elif any(word in title_words for word in ['nlp', 'language', 'text', 'linguistic']):\n",
    "        domain = \"natural language processing\"\n",
    "        visual_style = \"flowing text and language symbols transforming into digital patterns\"\n",
    "    elif any(word in title_words for word in ['robot', 'autonomous', 'control']):\n",
    "        domain = \"robotics\"\n",
    "        visual_style = \"sleek robotic elements and autonomous systems\"\n",
    "    elif any(word in title_words for word in ['data', 'analysis', 'mining']):\n",
    "        domain = \"data science\"\n",
    "        visual_style = \"abstract data visualizations and flowing information streams\"\n",
    "    else:\n",
    "        domain = \"artificial intelligence research\"\n",
    "        visual_style = \"abstract technological concepts with clean, modern design\"\n",
    "    \n",
    "    # Create a sophisticated prompt\n",
    "    prompt = f\"\"\"\n",
    "    Create a modern, professional illustration representing {domain} research. \n",
    "    The image should feature {visual_style}, using a clean and sophisticated \n",
    "    color palette with blues, purples, and subtle gradients. The style should be \n",
    "    minimalist yet engaging, suitable for a technology blog. Avoid text, people, \n",
    "    or specific logos. Focus on abstract technological concepts that convey \n",
    "    innovation and scientific progress.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"Generating DALL-E 3 image for: {paper.title[:50]}...\")\n",
    "        \n",
    "        response = client.images.generate(\n",
    "            model=IMAGE_MODEL,\n",
    "            prompt=prompt,\n",
    "            n=1,\n",
    "            size=IMAGE_SIZE,\n",
    "            quality=IMAGE_QUALITY,\n",
    "            style=IMAGE_STYLE\n",
    "        )\n",
    "        \n",
    "        image_url = response.data[0].url\n",
    "        print(\"‚úÖ DALL-E 3 image generated successfully!\")\n",
    "        \n",
    "        # Display the image in the notebook (if running in Jupyter)\n",
    "        try:\n",
    "            from IPython.display import display, Image\n",
    "            display(Image(url=image_url))\n",
    "        except ImportError:\n",
    "            print(f\"Image URL: {image_url}\")\n",
    "        \n",
    "        return image_url\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating DALL-E 3 image: {e}\")\n",
    "        \n",
    "        # Fallback: try with a simpler prompt\n",
    "        try:\n",
    "            print(\"Trying with simplified prompt...\")\n",
    "            simple_prompt = f\"A clean, modern illustration representing {domain}, minimalist style, blue and purple gradient background\"\n",
    "            \n",
    "            response = client.images.generate(\n",
    "                model=IMAGE_MODEL,\n",
    "                prompt=simple_prompt,\n",
    "                n=1,\n",
    "                size=IMAGE_SIZE,\n",
    "                quality=IMAGE_QUALITY\n",
    "            )\n",
    "            \n",
    "            image_url = response.data[0].url\n",
    "            print(\"‚úÖ Fallback image generated successfully!\")\n",
    "            return image_url\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå Fallback also failed: {e2}\")\n",
    "            return None\n",
    "\n",
    "def download_and_save_image(image_url, paper_short_id, date):\n",
    "    \"\"\"\n",
    "    Download the AI image and save it to GitHub repository\n",
    "    This prevents the image from expiring\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    import base64\n",
    "    \n",
    "    try:\n",
    "        # Download the image\n",
    "        print(\"Downloading image...\")\n",
    "        img_response = requests.get(image_url)\n",
    "        img_response.raise_for_status()\n",
    "        \n",
    "        # Create filename\n",
    "        image_filename = f\"assets/images/{date}-{paper_short_id}.png\"\n",
    "        \n",
    "        # Encode image for GitHub API\n",
    "        encoded_image = base64.b64encode(img_response.content).decode(\"utf-8\")\n",
    "        \n",
    "        # Upload to GitHub\n",
    "        url = f\"https://api.github.com/repos/{GITHUB_REPO}/contents/{image_filename}\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"token {GITHUB_TOKEN}\",\n",
    "            \"Accept\": \"application/vnd.github.v3+json\"\n",
    "        }\n",
    "        \n",
    "        data = {\n",
    "            \"message\": f\"Add image for blog post {paper_short_id}\",\n",
    "            \"content\": encoded_image\n",
    "        }\n",
    "        \n",
    "        response = requests.put(url, headers=headers, json=data)\n",
    "        \n",
    "        if response.status_code == 201:\n",
    "            # Return the GitHub-hosted image URL\n",
    "            github_image_url = f\"https://{GITHUB_PAGES_SITE}/{image_filename}\"\n",
    "            print(f\"‚úÖ Image saved to GitHub: {github_image_url}\")\n",
    "            \n",
    "            # Wait for GitHub Pages to deploy the image\n",
    "            print(f\"‚è≥ Waiting for GitHub Pages to deploy image ({GITHUB_PAGES_IMAGE_WAIT} seconds)...\")\n",
    "            time.sleep(GITHUB_PAGES_IMAGE_WAIT)\n",
    "            \n",
    "            # Test if the image is accessible\n",
    "            try:\n",
    "                test_response = requests.head(github_image_url, timeout=10)\n",
    "                if test_response.status_code == 200:\n",
    "                    print(\"‚úÖ GitHub Pages image is accessible!\")\n",
    "                    return github_image_url\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è GitHub Pages image not ready (status: {test_response.status_code})\")\n",
    "                    print(\"üìé Using original OpenAI URL as fallback\")\n",
    "                    return image_url\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Cannot verify GitHub Pages image accessibility: {e}\")\n",
    "                print(\"üìé Using original OpenAI URL as fallback\")\n",
    "                return image_url\n",
    "                \n",
    "        else:\n",
    "            print(f\"‚ùå Failed to save image to GitHub: {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            return image_url  # Return original URL as fallback\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving image: {e}\")\n",
    "        return image_url  # Return original URL as fallback\n",
    "\n",
    "def create_github_blog_post(paper, content, date, short_id, image_url):\n",
    "    # Use consistent short_id for the file name\n",
    "    file_name = f\"{date}-{short_id}.md\"\n",
    "    \n",
    "    # Generate Harvard reference\n",
    "    harvard_reference = generate_harvard_reference(paper)\n",
    "    \n",
    "    # Fixed: No indentation in front matter\n",
    "    file_content = f\"\"\"---\n",
    "layout: post\n",
    "title: \"{paper.title}\"\n",
    "date: {date} {datetime.now().strftime('%H:%M:%S +0000')}\n",
    "categories: [blog, AI, research]\n",
    "image: {image_url}\n",
    "---\n",
    "![AI Generated Image]({image_url})\n",
    "\n",
    "{content}\n",
    "\n",
    "## Original Research Paper\n",
    "For more details, please refer to the original research paper:\n",
    "[{paper.title}]({paper.entry_id})\n",
    "\n",
    "## Reference\n",
    "{harvard_reference}\n",
    "\"\"\"\n",
    "    \n",
    "    encoded_content = base64.b64encode(file_content.encode(\"utf-8\")).decode(\"utf-8\")\n",
    "    url = f\"https://api.github.com/repos/{GITHUB_REPO}/contents/_posts/{file_name}\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"token {GITHUB_TOKEN}\",\n",
    "        \"Accept\": \"application/vnd.github.v3+json\"\n",
    "    }\n",
    "    \n",
    "    # Check if file already exists\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        print(f\"Blog post already exists: {file_name}\")\n",
    "        return False, \"\"\n",
    "\n",
    "    # File doesn't exist, create new file\n",
    "    data = {\n",
    "        \"message\": f\"Add new blog post: {paper.title}\",\n",
    "        \"content\": encoded_content\n",
    "    }\n",
    "\n",
    "    response = requests.put(url, headers=headers, json=data)\n",
    "    if response.status_code != 201:\n",
    "        print(f\"GitHub API Error: {response.status_code}\")\n",
    "        print(f\"Response content: {response.text}\")\n",
    "        return False, \"\"\n",
    "    \n",
    "    # Construct the URL based on the file name\n",
    "    post_url = f\"https://{GITHUB_PAGES_SITE}/{date.replace('-', '/')}/{short_id}/\"\n",
    "    return True, post_url\n",
    "   \n",
    "\n",
    "def check_existing_post(short_id, date):\n",
    "    file_name = f\"{date}-{short_id}.md\"\n",
    "    url = f\"https://api.github.com/repos/{GITHUB_REPO}/contents/_posts/{file_name}\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"token {GITHUB_TOKEN}\",\n",
    "        \"Accept\": \"application/vnd.github.v3+json\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return response.status_code == 200\n",
    "\n",
    "def create_media_container(access_token, user_id, text, image_url):\n",
    "    url = f\"https://graph.threads.net/v1.0/{user_id}/threads\"\n",
    "    \n",
    "    params = {\n",
    "        \"media_type\": \"IMAGE\",\n",
    "        \"image_url\": image_url,\n",
    "        \"text\": text,\n",
    "        \"access_token\": access_token\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        print(f\"Create Media Container Status Code: {response.status_code}\")\n",
    "        return response.json()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error creating media container: {e}\")\n",
    "        if hasattr(e, 'response') and e.response is not None:\n",
    "            print(f\"Response content: {e.response.text}\")\n",
    "        return None\n",
    "\n",
    "def publish_thread(access_token, user_id, creation_id):\n",
    "    url = f\"https://graph.threads.net/v1.0/{user_id}/threads_publish\"\n",
    "    \n",
    "    params = {\n",
    "        \"creation_id\": creation_id,\n",
    "        \"access_token\": access_token\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        print(f\"Publish Thread Status Code: {response.status_code}\")\n",
    "        return response.json()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error publishing thread: {e}\")\n",
    "        if hasattr(e, 'response') and e.response is not None:\n",
    "            print(f\"Response content: {e.response.text}\")\n",
    "        return None\n",
    "\n",
    "def post_to_threads(text, image_url, access_token, user_id, initial_wait=30, max_retries=3):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Step 1: Create media container\n",
    "            container = create_media_container(access_token, user_id, text, image_url)\n",
    "            if container is None or 'id' not in container:\n",
    "                print(\"Failed to create media container.\")\n",
    "                return False\n",
    "\n",
    "            container_id = container['id']\n",
    "            print(f\"Media container created with ID: {container_id}\")\n",
    "\n",
    "            # Wait before publishing\n",
    "            print(f\"Waiting {initial_wait} seconds before publishing...\")\n",
    "            time.sleep(initial_wait)\n",
    "\n",
    "            # Step 2: Publish the thread\n",
    "            publish_result = publish_thread(access_token, user_id, container_id)\n",
    "            if publish_result is None or 'id' not in publish_result:\n",
    "                print(\"Failed to publish thread.\")\n",
    "                return False\n",
    "\n",
    "            print(f\"Successfully posted to Threads with ID: {publish_result['id']}\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error posting to Threads: {e}\")\n",
    "            \n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Retrying in {initial_wait} seconds...\")\n",
    "                time.sleep(initial_wait)\n",
    "            else:\n",
    "                print(\"Max retries reached. Failed to post to Threads.\")\n",
    "                return False\n",
    "\n",
    "    return False\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        if VERBOSE_OUTPUT:\n",
    "            print(\"üöÄ Starting automated blog creation process...\")\n",
    "            print(f\"üìù Configuration: {MAX_PAPERS_TO_PROCESS} papers, {DAYS_BACK_TO_SEARCH} days back\")\n",
    "            print(f\"üîç Categories: {', '.join(ARXIV_CATEGORIES)}\")\n",
    "        \n",
    "        papers = fetch_latest_papers()\n",
    "        if not papers:\n",
    "            print(f\"No recent papers found in categories: {ARXIV_CATEGORIES}\")\n",
    "            return\n",
    "\n",
    "        processed_count = 0\n",
    "        for paper in papers:\n",
    "            display(Markdown(f\"## Processing: {paper.title}\"))\n",
    "            \n",
    "            short_id = hashlib.md5(paper.title.encode()).hexdigest()[:8]\n",
    "            \n",
    "            date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "            if SKIP_EXISTING_POSTS and check_existing_post(short_id, date):\n",
    "                print(f\"Blog post already exists for: {paper.title}\")\n",
    "                print(\"Skipping to next paper...\")\n",
    "                continue\n",
    "\n",
    "            blog_post = generate_blog_post(paper)\n",
    "            if not blog_post:\n",
    "                print(f\"Failed to generate blog post for: {paper.title}\")\n",
    "                print(\"Skipping to next paper...\")\n",
    "                continue\n",
    "\n",
    "            display(Markdown(f\"### Original Paper: [{paper.entry_id}]({paper.entry_id})\"))\n",
    "            display(Markdown(blog_post))\n",
    "            \n",
    "            # Generate a temporary post URL\n",
    "            temp_post_url = f\"https://{GITHUB_PAGES_SITE}/{date.replace('-', '/')}/{short_id}/\"\n",
    "            \n",
    "            # Generate Threads post first\n",
    "            threads_post = generate_threads_post(paper, temp_post_url)\n",
    "            if not threads_post:\n",
    "                print(\"Failed to generate Threads post. Skipping to next paper...\")\n",
    "                continue\n",
    "            \n",
    "            # Generate image based on paper content\n",
    "            image_url = generate_ai_image(paper, threads_post)\n",
    "            if not image_url:\n",
    "                print(\"Failed to generate AI image. Skipping to next paper...\")\n",
    "                continue\n",
    "            \n",
    "            # Download and save the image to prevent expiration\n",
    "            if SAVE_IMAGES_TO_GITHUB:\n",
    "                image_url = download_and_save_image(image_url, short_id, date)\n",
    "            \n",
    "            # Create GitHub blog post with the generated image\n",
    "            success, post_url = create_github_blog_post(paper, blog_post, date, short_id, image_url)\n",
    "            \n",
    "            if success:\n",
    "                print(f\"Successfully created blog post on GitHub: {post_url}\")\n",
    "                \n",
    "                # Add delay to allow for link preview generation\n",
    "                print(f\"Waiting {LINK_PREVIEW_WAIT} seconds for link preview generation...\")\n",
    "                time.sleep(LINK_PREVIEW_WAIT)\n",
    "                \n",
    "                # Update the Threads post with the correct URL if it changed\n",
    "                if post_url != temp_post_url:\n",
    "                    threads_post = threads_post.replace(temp_post_url, post_url)\n",
    "                \n",
    "                display(Markdown(f\"### Threads Post:\\n{threads_post}\"))\n",
    "                if post_to_threads(threads_post, image_url, THREADS_ACCESS_TOKEN, THREADS_USER_ID, THREADS_WAIT_TIME, THREADS_MAX_RETRIES):\n",
    "                    print(\"Successfully posted to Threads with image!\")\n",
    "                else:\n",
    "                    print(\"Failed to post to Threads.\")\n",
    "                \n",
    "                processed_count += 1\n",
    "            else:\n",
    "                print(\"Failed to create blog post on GitHub.\")\n",
    "        \n",
    "        if VERBOSE_OUTPUT:\n",
    "            print(f\"\\n‚úÖ Processing complete! Created {processed_count} new blog posts.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
