{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241f7b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting automated blog creation process...\n",
      "📝 Configuration: 5 papers, 7 days back\n",
      "🔍 Categories: cs.AI, cs.LG, cs.CL, cs.CV, stat.ML\n",
      "An error occurred: fetch_latest_papers() missing 1 required positional argument: 'categories'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "fetch_latest_papers() missing 1 required positional argument: 'categories'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 527\u001b[0m\n\u001b[0;32m    524\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 527\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 450\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📝 Configuration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_PAPERS_TO_PROCESS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m papers, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDAYS_BACK_TO_SEARCH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m days back\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔍 Categories: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ARXIV_CATEGORIES)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 450\u001b[0m papers \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_latest_papers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m papers:\n\u001b[0;32m    452\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo recent papers found in categories: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mARXIV_CATEGORIES\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: fetch_latest_papers() missing 1 required positional argument: 'categories'"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import openai\n",
    "import requests\n",
    "import base64\n",
    "import time\n",
    "import hashlib\n",
    "import json\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from IPython.display import display, Markdown, Image\n",
    "from requests.exceptions import RequestException\n",
    "from io import BytesIO\n",
    "\n",
    "# Import secrets from separate file (not committed to git)\n",
    "from secrets_config import (\n",
    "    OPENAI_API_KEY,\n",
    "    THREADS_USER_ID,\n",
    "    THREADS_ACCESS_TOKEN,\n",
    "    APP_SECRET,\n",
    "    GITHUB_TOKEN,\n",
    "    GITHUB_REPO,\n",
    "    GITHUB_PAGES_SITE,\n",
    "    FACEBOOK_ID,\n",
    "    INSTAGRAM_ACCESS_TOKEN\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# BLOG AUTOMATION CONFIGURATION - Modify these as needed\n",
    "# ============================================================================\n",
    "\n",
    "# arXiv paper settings\n",
    "ARXIV_CATEGORIES = [\"cs.AI\", \"cs.LG\", \"cs.CL\", \"cs.CV\", \"stat.ML\"]\n",
    "MAX_PAPERS_TO_PROCESS = 5       # Number of papers to fetch and potentially process\n",
    "DAYS_BACK_TO_SEARCH = 7         # How many days back to search for new papers\n",
    "\n",
    "# Content generation settings\n",
    "BLOG_POST_LENGTH = \"300-400\"    # Target word count for blog posts\n",
    "BLOG_POST_MAX_TOKENS = 500      # Max tokens for GPT response\n",
    "BLOG_POST_TEMPERATURE = 0.7     # Creativity level (0.0-1.0)\n",
    "\n",
    "# Threads post settings\n",
    "THREADS_MAX_CHARS = 350         # Max characters for main Threads text\n",
    "THREADS_HASHTAGS = \"#AI #ArtificialIntelligence #MachineLearning #DataScience #Latest #Research #Arxiv #OpenAI\"\n",
    "THREADS_WAIT_TIME = 30          # Seconds to wait before publishing\n",
    "THREADS_MAX_RETRIES = 3\n",
    "\n",
    "# Image generation settings (DALL-E 3)\n",
    "IMAGE_MODEL = \"dall-e-3\"        # \"dall-e-2\" or \"dall-e-3\"\n",
    "IMAGE_QUALITY = \"standard\"      # \"standard\" or \"hd\" (hd costs more)\n",
    "IMAGE_STYLE = \"natural\"         # \"natural\" or \"vivid\"\n",
    "IMAGE_SIZE = \"1024x1024\"        # \"1024x1024\", \"1792x1024\", or \"1024x1792\"\n",
    "\n",
    "# Processing settings\n",
    "SKIP_EXISTING_POSTS = True      # Skip papers that already have blog posts\n",
    "SAVE_IMAGES_TO_GITHUB = True    # Download and save images to prevent expiration\n",
    "LINK_PREVIEW_WAIT = 30          # Seconds to wait for link preview generation\n",
    "\n",
    "# Testing/debugging settings\n",
    "TEST_MODE = False               # Set to True to process only 1 paper for testing\n",
    "VERBOSE_OUTPUT = True           # Show detailed processing information\n",
    "\n",
    "# ============================================================================\n",
    "# Auto-adjust settings based on test mode\n",
    "# ============================================================================\n",
    "if TEST_MODE:\n",
    "    MAX_PAPERS_TO_PROCESS = 1\n",
    "    print(\"🧪 TEST MODE ENABLED - Processing only 1 paper\")\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def generate_harvard_reference(paper):\n",
    "    authors = paper.authors\n",
    "    \n",
    "    if len(authors) == 1:\n",
    "        author_str = authors[0].name\n",
    "    elif len(authors) == 2:\n",
    "        author_str = f\"{authors[0].name} and {authors[1].name}\"\n",
    "    else:\n",
    "        author_str = f\"{authors[0].name} et al.\"\n",
    "    \n",
    "    year = paper.published.year\n",
    "    title = paper.title\n",
    "    \n",
    "    reference = f\"{author_str} ({year}) '{title}', arXiv preprint arXiv:{paper.get_short_id()}.\"\n",
    "    \n",
    "    return reference\n",
    "\n",
    "def fetch_latest_papers(categories=None, max_results=None, days_back=None):\n",
    "    # Use config defaults if no parameters provided\n",
    "    if categories is None:\n",
    "        categories = ARXIV_CATEGORIES\n",
    "    if max_results is None:\n",
    "        max_results = MAX_PAPERS_TO_PROCESS\n",
    "    if days_back is None:\n",
    "        days_back = DAYS_BACK_TO_SEARCH\n",
    "        \n",
    "    client_arxiv = arxiv.Client()  # Renamed to avoid confusion with OpenAI client\n",
    "    cutoff_date = datetime.now(timezone.utc) - timedelta(days=days_back)\n",
    "    \n",
    "    category_query = \" OR \".join([f\"cat:{cat}\" for cat in categories])\n",
    "    \n",
    "    search = arxiv.Search(\n",
    "        query = f\"({category_query})\",\n",
    "        max_results = max_results,\n",
    "        sort_by = arxiv.SortCriterion.SubmittedDate\n",
    "    )\n",
    "    \n",
    "    results = list(client_arxiv.results(search))\n",
    "    recent_papers = [paper for paper in results if paper.published.replace(tzinfo=timezone.utc) > cutoff_date]\n",
    "    \n",
    "    if VERBOSE_OUTPUT:\n",
    "        print(f\"📊 Fetched {len(results)} papers, {len(recent_papers)} from last {days_back} days\")\n",
    "    \n",
    "    return recent_papers\n",
    "\n",
    "def generate_blog_post(paper):\n",
    "    authors = ', '.join([author.name for author in paper.authors])\n",
    "    prompt = f\"\"\"Write an engaging blog post about the following scientific paper:\n",
    "\n",
    "Title: {paper.title}\n",
    "Authors: {authors}\n",
    "Abstract: {paper.summary}\n",
    "\n",
    "The blog post should:\n",
    "1. Explain the main findings in simple terms\n",
    "2. Discuss potential real-world implications\n",
    "3. Be engaging and accessible to a general audience\n",
    "4. Be around {BLOG_POST_LENGTH} words long\n",
    "\n",
    "Blog Post:\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that writes engaging blog posts about scientific papers.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=BLOG_POST_MAX_TOKENS,\n",
    "            temperature=BLOG_POST_TEMPERATURE\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating blog post: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_threads_post(paper, blog_post_url):\n",
    "    prompt = f\"\"\"Create a short, engaging post for Threads (max {THREADS_MAX_CHARS} characters) about this scientific paper:\n",
    "    Title: {paper.title}\n",
    "    \n",
    "    Include a brief highlight of the research and its potential impact. \n",
    "    Do not include any hashtags or 'Read more' statements.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that creates engaging social media posts about scientific papers.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=100,\n",
    "            temperature=BLOG_POST_TEMPERATURE\n",
    "        )\n",
    "        threads_text = response.choices[0].message.content.strip().replace(\":\", \"\")\n",
    "        \n",
    "        full_post = f\"{threads_text}\\n\\n{THREADS_HASHTAGS}\\n\\nRead more: {blog_post_url}\"\n",
    "        \n",
    "        if len(full_post) > 500:\n",
    "            available_chars = 500 - len(THREADS_HASHTAGS) - len(blog_post_url) - 15\n",
    "            truncated_text = threads_text[:available_chars-3] + \"...\"\n",
    "            full_post = f\"{truncated_text}\\n\\n{THREADS_HASHTAGS}\\n\\nRead more: {blog_post_url}\"\n",
    "        \n",
    "        return full_post\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating Threads post: {e}\")\n",
    "        return None\n",
    "   \n",
    "def generate_ai_image(paper, threads_post):\n",
    "    \"\"\"\n",
    "    Generate an AI image using DALL-E 3 based on the research paper\n",
    "    \"\"\"\n",
    "    # Create a more specific prompt based on the paper's content\n",
    "    # Extract key concepts from the paper title and abstract\n",
    "    title_words = paper.title.lower()\n",
    "    \n",
    "    # Determine the research domain for better image prompts\n",
    "    if any(word in title_words for word in ['neural', 'network', 'deep', 'learning', 'ai', 'artificial']):\n",
    "        domain = \"neural networks and AI\"\n",
    "        visual_style = \"futuristic digital networks with glowing nodes and connections\"\n",
    "    elif any(word in title_words for word in ['computer', 'vision', 'image', 'visual']):\n",
    "        domain = \"computer vision\"\n",
    "        visual_style = \"digital image processing with geometric patterns and visual data\"\n",
    "    elif any(word in title_words for word in ['nlp', 'language', 'text', 'linguistic']):\n",
    "        domain = \"natural language processing\"\n",
    "        visual_style = \"flowing text and language symbols transforming into digital patterns\"\n",
    "    elif any(word in title_words for word in ['robot', 'autonomous', 'control']):\n",
    "        domain = \"robotics\"\n",
    "        visual_style = \"sleek robotic elements and autonomous systems\"\n",
    "    elif any(word in title_words for word in ['data', 'analysis', 'mining']):\n",
    "        domain = \"data science\"\n",
    "        visual_style = \"abstract data visualizations and flowing information streams\"\n",
    "    else:\n",
    "        domain = \"artificial intelligence research\"\n",
    "        visual_style = \"abstract technological concepts with clean, modern design\"\n",
    "    \n",
    "    # Create a sophisticated prompt\n",
    "    prompt = f\"\"\"\n",
    "    Create a modern, professional illustration representing {domain} research. \n",
    "    The image should feature {visual_style}, using a clean and sophisticated \n",
    "    color palette with blues, purples, and subtle gradients. The style should be \n",
    "    minimalist yet engaging, suitable for a technology blog. Avoid text, people, \n",
    "    or specific logos. Focus on abstract technological concepts that convey \n",
    "    innovation and scientific progress.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"Generating DALL-E 3 image for: {paper.title[:50]}...\")\n",
    "        \n",
    "        response = client.images.generate(\n",
    "            model=IMAGE_MODEL,\n",
    "            prompt=prompt,\n",
    "            n=1,\n",
    "            size=IMAGE_SIZE,\n",
    "            quality=IMAGE_QUALITY,\n",
    "            style=IMAGE_STYLE\n",
    "        )\n",
    "        \n",
    "        image_url = response.data[0].url\n",
    "        print(\"✅ DALL-E 3 image generated successfully!\")\n",
    "        \n",
    "        # Display the image in the notebook (if running in Jupyter)\n",
    "        try:\n",
    "            from IPython.display import display, Image\n",
    "            display(Image(url=image_url))\n",
    "        except ImportError:\n",
    "            print(f\"Image URL: {image_url}\")\n",
    "        \n",
    "        return image_url\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating DALL-E 3 image: {e}\")\n",
    "        \n",
    "        # Fallback: try with a simpler prompt\n",
    "        try:\n",
    "            print(\"Trying with simplified prompt...\")\n",
    "            simple_prompt = f\"A clean, modern illustration representing {domain}, minimalist style, blue and purple gradient background\"\n",
    "            \n",
    "            response = client.images.generate(\n",
    "                model=IMAGE_MODEL,\n",
    "                prompt=simple_prompt,\n",
    "                n=1,\n",
    "                size=IMAGE_SIZE,\n",
    "                quality=IMAGE_QUALITY\n",
    "            )\n",
    "            \n",
    "            image_url = response.data[0].url\n",
    "            print(\"✅ Fallback image generated successfully!\")\n",
    "            return image_url\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"❌ Fallback also failed: {e2}\")\n",
    "            return None\n",
    "\n",
    "def download_and_save_image(image_url, paper_short_id, date):\n",
    "    \"\"\"\n",
    "    Download the AI image and save it to GitHub repository\n",
    "    This prevents the image from expiring\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    import base64\n",
    "    \n",
    "    try:\n",
    "        # Download the image\n",
    "        print(\"Downloading image...\")\n",
    "        img_response = requests.get(image_url)\n",
    "        img_response.raise_for_status()\n",
    "        \n",
    "        # Create filename\n",
    "        image_filename = f\"assets/images/{date}-{paper_short_id}.png\"\n",
    "        \n",
    "        # Encode image for GitHub API\n",
    "        encoded_image = base64.b64encode(img_response.content).decode(\"utf-8\")\n",
    "        \n",
    "        # Upload to GitHub\n",
    "        url = f\"https://api.github.com/repos/{GITHUB_REPO}/contents/{image_filename}\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"token {GITHUB_TOKEN}\",\n",
    "            \"Accept\": \"application/vnd.github.v3+json\"\n",
    "        }\n",
    "        \n",
    "        data = {\n",
    "            \"message\": f\"Add image for blog post {paper_short_id}\",\n",
    "            \"content\": encoded_image\n",
    "        }\n",
    "        \n",
    "        response = requests.put(url, headers=headers, json=data)\n",
    "        \n",
    "        if response.status_code == 201:\n",
    "            # Return the GitHub-hosted image URL\n",
    "            github_image_url = f\"https://{GITHUB_PAGES_SITE}/{image_filename}\"\n",
    "            print(f\"✅ Image saved to GitHub: {github_image_url}\")\n",
    "            return github_image_url\n",
    "        else:\n",
    "            print(f\"❌ Failed to save image to GitHub: {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            return image_url  # Return original URL as fallback\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving image: {e}\")\n",
    "        return image_url  # Return original URL as fallback\n",
    "\n",
    "def create_github_blog_post(paper, content, date, short_id, image_url):\n",
    "    # Use consistent short_id for the file name\n",
    "    file_name = f\"{date}-{short_id}.md\"\n",
    "    \n",
    "    # Generate Harvard reference\n",
    "    harvard_reference = generate_harvard_reference(paper)\n",
    "    \n",
    "    # Fixed: No indentation in front matter\n",
    "    file_content = f\"\"\"---\n",
    "layout: post\n",
    "title: \"{paper.title}\"\n",
    "date: {date} {datetime.now().strftime('%H:%M:%S +0000')}\n",
    "categories: [blog, AI, research]\n",
    "image: {image_url}\n",
    "---\n",
    "![AI Generated Image]({image_url})\n",
    "\n",
    "{content}\n",
    "\n",
    "## Original Research Paper\n",
    "For more details, please refer to the original research paper:\n",
    "[{paper.title}]({paper.entry_id})\n",
    "\n",
    "## Reference\n",
    "{harvard_reference}\n",
    "\"\"\"\n",
    "    \n",
    "    encoded_content = base64.b64encode(file_content.encode(\"utf-8\")).decode(\"utf-8\")\n",
    "    url = f\"https://api.github.com/repos/{GITHUB_REPO}/contents/_posts/{file_name}\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"token {GITHUB_TOKEN}\",\n",
    "        \"Accept\": \"application/vnd.github.v3+json\"\n",
    "    }\n",
    "    \n",
    "    # Check if file already exists\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        print(f\"Blog post already exists: {file_name}\")\n",
    "        return False, \"\"\n",
    "\n",
    "    # File doesn't exist, create new file\n",
    "    data = {\n",
    "        \"message\": f\"Add new blog post: {paper.title}\",\n",
    "        \"content\": encoded_content\n",
    "    }\n",
    "\n",
    "    response = requests.put(url, headers=headers, json=data)\n",
    "    if response.status_code != 201:\n",
    "        print(f\"GitHub API Error: {response.status_code}\")\n",
    "        print(f\"Response content: {response.text}\")\n",
    "        return False, \"\"\n",
    "    \n",
    "    # Construct the URL based on the file name\n",
    "    post_url = f\"https://{GITHUB_PAGES_SITE}/{date.replace('-', '/')}/{short_id}/\"\n",
    "    return True, post_url\n",
    "   \n",
    "\n",
    "def check_existing_post(short_id, date):\n",
    "    file_name = f\"{date}-{short_id}.md\"\n",
    "    url = f\"https://api.github.com/repos/{GITHUB_REPO}/contents/_posts/{file_name}\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"token {GITHUB_TOKEN}\",\n",
    "        \"Accept\": \"application/vnd.github.v3+json\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return response.status_code == 200\n",
    "\n",
    "def create_media_container(access_token, user_id, text, image_url):\n",
    "    url = f\"https://graph.threads.net/v1.0/{user_id}/threads\"\n",
    "    \n",
    "    params = {\n",
    "        \"media_type\": \"IMAGE\",\n",
    "        \"image_url\": image_url,\n",
    "        \"text\": text,\n",
    "        \"access_token\": access_token\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        print(f\"Create Media Container Status Code: {response.status_code}\")\n",
    "        return response.json()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error creating media container: {e}\")\n",
    "        if hasattr(e, 'response') and e.response is not None:\n",
    "            print(f\"Response content: {e.response.text}\")\n",
    "        return None\n",
    "\n",
    "def publish_thread(access_token, user_id, creation_id):\n",
    "    url = f\"https://graph.threads.net/v1.0/{user_id}/threads_publish\"\n",
    "    \n",
    "    params = {\n",
    "        \"creation_id\": creation_id,\n",
    "        \"access_token\": access_token\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        print(f\"Publish Thread Status Code: {response.status_code}\")\n",
    "        return response.json()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error publishing thread: {e}\")\n",
    "        if hasattr(e, 'response') and e.response is not None:\n",
    "            print(f\"Response content: {e.response.text}\")\n",
    "        return None\n",
    "\n",
    "def post_to_threads(text, image_url, access_token, user_id, initial_wait=30, max_retries=3):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Step 1: Create media container\n",
    "            container = create_media_container(access_token, user_id, text, image_url)\n",
    "            if container is None or 'id' not in container:\n",
    "                print(\"Failed to create media container.\")\n",
    "                return False\n",
    "\n",
    "            container_id = container['id']\n",
    "            print(f\"Media container created with ID: {container_id}\")\n",
    "\n",
    "            # Wait before publishing\n",
    "            print(f\"Waiting {initial_wait} seconds before publishing...\")\n",
    "            time.sleep(initial_wait)\n",
    "\n",
    "            # Step 2: Publish the thread\n",
    "            publish_result = publish_thread(access_token, user_id, container_id)\n",
    "            if publish_result is None or 'id' not in publish_result:\n",
    "                print(\"Failed to publish thread.\")\n",
    "                return False\n",
    "\n",
    "            print(f\"Successfully posted to Threads with ID: {publish_result['id']}\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error posting to Threads: {e}\")\n",
    "            \n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Retrying in {initial_wait} seconds...\")\n",
    "                time.sleep(initial_wait)\n",
    "            else:\n",
    "                print(\"Max retries reached. Failed to post to Threads.\")\n",
    "                return False\n",
    "\n",
    "    return False\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        if VERBOSE_OUTPUT:\n",
    "            print(\"🚀 Starting automated blog creation process...\")\n",
    "            print(f\"📝 Configuration: {MAX_PAPERS_TO_PROCESS} papers, {DAYS_BACK_TO_SEARCH} days back\")\n",
    "            print(f\"🔍 Categories: {', '.join(ARXIV_CATEGORIES)}\")\n",
    "        \n",
    "        papers = fetch_latest_papers()\n",
    "        if not papers:\n",
    "            print(f\"No recent papers found in categories: {ARXIV_CATEGORIES}\")\n",
    "            return\n",
    "\n",
    "        processed_count = 0\n",
    "        for paper in papers:\n",
    "            display(Markdown(f\"## Processing: {paper.title}\"))\n",
    "            \n",
    "            short_id = hashlib.md5(paper.title.encode()).hexdigest()[:8]\n",
    "            \n",
    "            date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "            if SKIP_EXISTING_POSTS and check_existing_post(short_id, date):\n",
    "                print(f\"Blog post already exists for: {paper.title}\")\n",
    "                print(\"Skipping to next paper...\")\n",
    "                continue\n",
    "\n",
    "            blog_post = generate_blog_post(paper)\n",
    "            if not blog_post:\n",
    "                print(f\"Failed to generate blog post for: {paper.title}\")\n",
    "                print(\"Skipping to next paper...\")\n",
    "                continue\n",
    "\n",
    "            display(Markdown(f\"### Original Paper: [{paper.entry_id}]({paper.entry_id})\"))\n",
    "            display(Markdown(blog_post))\n",
    "            \n",
    "            # Generate a temporary post URL\n",
    "            temp_post_url = f\"https://{GITHUB_PAGES_SITE}/{date.replace('-', '/')}/{short_id}/\"\n",
    "            \n",
    "            # Generate Threads post first\n",
    "            threads_post = generate_threads_post(paper, temp_post_url)\n",
    "            if not threads_post:\n",
    "                print(\"Failed to generate Threads post. Skipping to next paper...\")\n",
    "                continue\n",
    "            \n",
    "            # Generate image based on paper content\n",
    "            image_url = generate_ai_image(paper, threads_post)\n",
    "            if not image_url:\n",
    "                print(\"Failed to generate AI image. Skipping to next paper...\")\n",
    "                continue\n",
    "            \n",
    "            # Download and save the image to prevent expiration\n",
    "            if SAVE_IMAGES_TO_GITHUB:\n",
    "                image_url = download_and_save_image(image_url, short_id, date)\n",
    "            \n",
    "            # Create GitHub blog post with the generated image\n",
    "            success, post_url = create_github_blog_post(paper, blog_post, date, short_id, image_url)\n",
    "            \n",
    "            if success:\n",
    "                print(f\"Successfully created blog post on GitHub: {post_url}\")\n",
    "                \n",
    "                # Add delay to allow for link preview generation\n",
    "                print(f\"Waiting {LINK_PREVIEW_WAIT} seconds for link preview generation...\")\n",
    "                time.sleep(LINK_PREVIEW_WAIT)\n",
    "                \n",
    "                # Update the Threads post with the correct URL if it changed\n",
    "                if post_url != temp_post_url:\n",
    "                    threads_post = threads_post.replace(temp_post_url, post_url)\n",
    "                \n",
    "                display(Markdown(f\"### Threads Post:\\n{threads_post}\"))\n",
    "                if post_to_threads(threads_post, image_url, THREADS_ACCESS_TOKEN, THREADS_USER_ID, THREADS_WAIT_TIME, THREADS_MAX_RETRIES):\n",
    "                    print(\"Successfully posted to Threads with image!\")\n",
    "                else:\n",
    "                    print(\"Failed to post to Threads.\")\n",
    "                \n",
    "                processed_count += 1\n",
    "            else:\n",
    "                print(\"Failed to create blog post on GitHub.\")\n",
    "        \n",
    "        if VERBOSE_OUTPUT:\n",
    "            print(f\"\\n✅ Processing complete! Created {processed_count} new blog posts.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
